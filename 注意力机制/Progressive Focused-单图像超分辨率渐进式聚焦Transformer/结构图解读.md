### 1. 整体网络架构 (Figure 2, 上半部分)

Figure 2 的上半部分展示了 PFT 模型的完整数据流。这是一个基于 Transformer 的典型超分辨率（SR）架构，但其核心被替换为了创新的 PFA 模块。

![结构图](https://gitee.com/ChadHui/typora-image/raw/master/cv-image/20251103210519.jpg)

**其详细数据流如下**：

1.  **浅层特征提取 (Shallow Feature Extraction)**：
    * 首先，输入的低分辨率（LR）图像进入一个标准的卷积层（`Conv`），用于提取初始的浅层特征。

2.  **核心模块 (Deep Feature Extraction)**：
    * 浅层特征被送入第一个 `PFA Block` 进行深层特征提取。
    * 紧接着，特征会进入一个包含 `M` 个**渐进式聚焦注意力层 (PFAL)** 的堆叠模块（`PFAL × M`）。
    * 如图所示，每个 `PFAL` 内部都包含 `LayerNorm`、`PFA` 核心注意力模块、`LayerNorm` 和一个 `MLP` 层，并辅以残差连接（Residual Connection）。这是 Transformer Block 的一个变体。
    * 在 `M` 层堆叠后，特征会再经过一个 `PFA Block` 和一个 `Conv` 层 进行最终的特征融合。

3.  **全局残差连接 (Global Residual Connection)**：
    * 一个非常关键的长跳跃连接（图中的长弯箭头）将第一个 `Conv` 层输出的*浅层特征*与最后一个 `Conv` 层输出的*深层特征*进行逐元素相加（`+`）。这是 SR 领域中一个经典且有效的设计，它允许网络将原始的低频信息直接传递到末端，使得深层模块可以更专注于学习高频的残差细节。

4.  **图像重建 (Image Reconstruction)**：
    * 最后，融合后的特征被送入一个 `Upsample`（上采样）模块，将特征图的分辨率提升到目标尺寸，从而重建出高分辨率（HR）图像。

### 2. 核心创新模块详解 (Figure 2, 下半部分)

Figure 2 的下半部分 是这篇论文的**精髓**，它详细拆解了 `PFA`（Progressive Focused Attention）模块的内部工作机制。与标准自注意力不同，它引入了一个“跨层状态”——`PFA maps of previous layer`。

我们来详细追踪这个模块的数据流，特别是 `PFA maps of previous layer` 所扮演的**两个核心角色**：

#### 角色一：计算预过滤 (Computation Pre-filtering)

1.  **输入**：模块接收标准的 `Q` (Query), `K^T` (Key Transposed), `V` (Value) 矩阵，以及来自上一层（或上一个 Block）的注意力图 `PFA maps of previous layer`。
2.  **稀疏矩阵乘法 (SMM)**：`PFA maps of previous layer` **首先**被用作一个*索引掩码*（Index Mask），送入第一个 `SMM` (Sparse Matrix Multiplication) 模块。
3.  **工作机制**：`SMM` 模块依据这个掩码，在计算 `Q` 和 `K^T` 的相似度时，**只计算**掩码中标记为“相关”的稀疏位置。换言之，它**跳过**了（Pre-filter）大量上一层就已经认为不相关的 Token 对的相似度计算。
4.  **输出**：SMM 的输出是一个稀疏的、被选择性计算出来的 `Calculated attention maps`（已计算的注意力图）。

> **分析**：这是 PFA 高效性的关键。传统注意力（如 Figure 1a）和稀疏注意力（如 Figure 1b）都需要计算*全部*的 $Q \times K^T$ 相似度，然后再进行筛选。而 PFA 在计算*之前*就已经利用先验知识过滤掉了大部分计算，极大地降低了计算复杂度。

#### 角色二：注意力继承与提纯 (Attention Inheritance and Refinement)

1.  **Hadamard multiplication**：`PFA maps of previous layer` **同时**被送入一个 `Hadamard multiplication`（逐元素乘法）节点。
2.  **工作机制**：它与上一步 SMM 得到的 `Calculated attention maps` 进行逐元素相乘。
3.  **效果**：这个乘法操作实现了注意力的“**继承**”。
    * 如果一个 Token 在*上一层*的权重很低（例如 0.1），即使在*本层*计算出的相似度很高（例如 0.9），相乘后的权重也会被抑制（$0.1 \times 0.9 = 0.09$）。
    * 只有那些在**连续多层**中都保持高相似度的 Token，才能在连乘中存活并被放大。
    * 这就实现了对注意力权重的“**提纯**”（Refinement），使得模型能逐渐聚焦到最关键的特征上。

#### 模块的后续流程

1.  **稀疏化 (Sparsification)**：Hadamard 积的结果被送入 `Sparse` 模块，即 Top-k 选择器，它会选出每行中 K 个最大的注意力值，进一步强化稀疏性。
2.  **状态传递**：`Sparse` 模块的输出即为 `Focused attention maps`（聚焦注意力图），它有两个去向：
    * **(A)** 作为 `PFA maps of current layer`（当前层 PFA 图），传递给*下一个* PFA 模块，继续扮演“预过滤”和“继承”的角色。
    * **(B)** 送入第二个 `SMM` 模块。
3.  **特征聚合**：在第二个 `SMM` 模块中，这个 `Focused attention maps` 与 `V` 矩阵相乘，依据提纯后的稀疏权重来聚合 Value 特征，最终得到输出 `O`。

**总结**：Figure 2 详细地展示了 PFT 架构。其核心 PFA 模块通过巧妙地跨层传递注意力图（`PFA maps`），同时实现了“**计算预过滤**”（通过 SMM 降低成本）和“**注意力提纯**”（通过 Hadamard 积提升精度），从而解决了传统注意力的效率和效果瓶颈。