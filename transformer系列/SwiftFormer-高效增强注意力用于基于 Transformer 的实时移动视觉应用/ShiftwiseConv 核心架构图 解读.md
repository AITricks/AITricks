# ShiftwiseConv 核心架构图 (Fig 1-4) 解读

---

### 1. 动机图解 (Figure 1 & 4)

这组图表揭示了 SW Conv 的生物学灵感和其设计的优化动机。

* **Figure 1 (left panel): 生物学灵感——视网膜的解耦设计**
    * 这张图是人眼视网膜的细胞结构图。作者以此为灵感，来说明生物视觉系统是如何处理长距离依赖的。
    * **解耦 (Decoupling)**：视网膜的信号处理被解耦为两个部分：
        1.  **粒度提取**：由“感光细胞”（如视杆细胞 `Rod` 和视锥细胞 `Cone`）完成基础的信号捕捉。
        2.  **多路径融合**：信号并不直接传递，而是通过一个复杂的细胞网络（如“水平细胞” `Horizontal cell`、“双极细胞” `Bipolar cell` 等）进行**多条路径**的整合，然后传递给神经节细胞。
    * **启发**：这启发作者，CNNs 也可以模仿这种解耦，将大卷积核 (LK) 的作用拆分为：“**基础粒度（原子性）的特征提取**”和“**通过多样化连接（多路径）的特征融合**”。

* **Figure 4 (a) & (b): 优化动机——提升特征利用率**
    * 这两张图是SW Conv 模块的设计动机之一，用于说明为什么需要更复杂的“多路径”设计。
    * **Figure 4(a)** 展示了基础 SW 操作的特征图覆盖率。可见覆盖是可预测的，且利用率有限（例如，图中显示了 83%、94% 等，但很多区域未被充分利用）。
    * **Figure 4(b)** 是一个消融实验图，展示了“特征利用率” (Feature utilization rate) 如何随“路径数” (`N paths`) 变化。
        * `w/o` (without shuffled ordering) 曲线（虚线）显示，即使增加路径数，如果顺序是固定的，利用率的提升也很有限。
        * `w/` (with shuffled ordering) 曲线（实线）显示，如果路径的融合顺序是**随机打乱**的，利用率会随着路径数的增加而**显著提高**。
    * **启发**：这证明了简单的多路径（如 Figure 1 右侧的基础版）是不够的，模块需要引入**随机性**或**多样性**（如 `disordered offset` 或 `additional edges`）来打破可预测性，从而最大化特征融合的效率。

### 2. 整体网络架构 (Figure 1 right panel & 3a)

Figure 1 的右侧面板展示了最终 SW Conv 模块的宏观架构，而 Figure 3(a) 展示了其内部的一个关键变体。

* **Figure 1 (right panel): SW Conv 宏观架构**
    * 这是一个“以连接为中心”的模块。
    * **数据流**：
        1.  输入特征首先进入一个 `group Conv`（$3 \times 3$ 组卷积）。这就是 Figure 1 左侧启发的“**粒度提取**”。
        2.  组卷积的输出被分成**多条路径**，这些路径是“稀疏的”。
        3.  不同的路径经过不同的处理：
            * **`Rep` 路径**：一条或多条路径通过 `Rep`（重参数化）分支。
            * **`SW` 路径**：其他路径通过 `SW`（Shiftwise，即移位）操作。
            * **剪枝**：图中 `pruned`（虚线框） 和 `X` (Fig 3a) 标记表明，一些连接（即组卷积的输出通道）在训练中会被剪枝，以实现数据驱动的稀疏连接。
        4.  **融合**：所有路径的输出在各自通过 `BN(sum(...))` 后，最终被相加 (`+`) 在一起，完成“**多路径融合**”。

* **Figure 3(a): 消除冗余的变体**
    * 该图展示了对 SLaK 中双分支（$M \times N$ 和 $N \times M$）冗余性的一种优化。
    * 它只使用**一个 `group Conv`** 的输出。
    * 为了制造差异性，这个输出被 `sampling`（采样）到多条路径：
        1.  **`layout` 路径**：一条常规的移位平铺路径。
        2.  **`reverse` 路径**：另一条路径在 `layout` 后再施加一个 `reverse`（反向）移位，以避免相关性。
        3.  **`sampling` 路径**：一条未经移位的旁路（可能用于 `Rep` 或 $1 \times 1$ 卷积）。
    * **BN 重新定位**：该图还展示了一个关键改动：`BN` 层从卷积后移到了**移位操作之后**。原因是：移位（SW）后的数据（来自不同空间位置）差异度高，需要BN；而重参数化（Rep）分支的数据在移位前差异度低。

### 3. 核心创新模块详解 (Figure 2 & 3)

这组图表是本文方法论的演进核心，展示了“如何用 $3 \times 3$ 卷积等效替代大核”。

![结构图2](https://gitee.com/ChadHui/typora-image/raw/master/cv-image/20251031144502.jpg)

* **Figure 2 (a) vs (b): 核心等效替换**
    * **Figure 2(a) SLaK**：展示了基线 SLaK 架构。它依赖于两个并行的、巨大的**条带卷积**（$M \times N$ 和 $N \times M$）。
    * **Figure 2(b) SW'**: 展示了本文的**核心洞察**。它指出，SLaK 的 $M \times N$ 条带卷积（图 2a）在数学上可以**等效**为：
        1.  一系列 $N \times N$ 的 `group Conv`（组卷积，g=C）。
        2.  然后对组卷积的 $\lceil M/N \rceil$ 个输出通道（路径）施加一个 `shift Op.`（移位操作）。
    * 这个“替换实验”是本文的立论之本，证明了 $M \times N$ 大核**并非不可或缺**。

* **Figure 2(c): `shift Op.` (移位操作) 详解**
    * 该图揭示了图 2(b) 中 `shift Op.` 的内部机制。
    * **输入**：来自 $N \times N$ 组卷积的 $K$ 个通道（$K=\lceil M/N \rceil$），这里表示为 `KHW`。
    * **操作**：`clip`（裁剪/偏移）操作。它对 $K$ 个通道中的第 $k$ 个通道施加一个 $(k-1) \times N$ 的空间偏移，从而将 $K$ 个 $N \times N$ 的小核在空间上“平铺”(layout) 开，使其覆盖范围等效于一个 $M \times N$ 的大核。
    * **`+` (相加)**：移位后的特征图被相加。这个加法确保了即使在粗粒度剪枝（`coarse-grained pruning`）（即丢弃 K 个通道中的某几个）之后，网络的结构完整性依然保持不变。

* **Figure 3(b): 降低参数的“瘦身” (Slimming) 策略**
    
    ![结构图3](https://gitee.com/ChadHui/typora-image/raw/master/cv-image/20251031144536.jpg)
    
    * 该图展示了在实现等效替换后，如何解决 SLaK 带来的“网络宽度膨胀”问题。
    * **Ghost-like 方法**：它采用了一种类似 GhostNet 的“瘦身”技术。
    * **数据流**：
        1.  输入通道被分成两部分（蓝色和橙色）。
        2.  **蓝色部分**：作为“Ghost”特征，被**直接旁路**到输出端。
        3.  **橙色部分**：被送入昂贵的 `shift-wise` 模块（包含 `Rep`）进行计算。
        4.  **`cat` (拼接)**：计算后的橙色特征与旁路的蓝色特征在通道维度 `cat`（拼接），得到最终输出。
    * 这使得模型可以在保持输入/输出通道数不变的同时，**显著减少**真正参与大核效应计算的参数量。

### 4. 图解总结

这四组图（Fig 1, 2, 3, 4）共同讲述了一个完整的、从“灵感”到“实现”再到“优化”的故事：

1.  **灵感 (Fig 1 )**：人眼视网膜的“解耦”设计 启发作者，大核效应应被拆分为“**粒度提取**”和“**多路径融合**”。
2.  **核心发现 (Fig 2)**：论文的“尤里卡时刻”。作者通过 Fig 2(b) 证明了 SLaK 的 $M \times N$ 大核（Fig 2a）可以被 $N \times N$ 组卷积 + `shift Op.`（Fig 2c）**完美等效替换**。
3.  **优化 (Fig 3 & 4)**：在“替换”成功后，作者进行了优化：
    * **效率优化 (Fig 3a)**：合并冗余分支，使用 `reverse` 移位创造差异。
    * **参数优化 (Fig 3b)**：使用 `ghost-like` 结构“瘦身”，减少计算量。
    * **性能优化 (Fig 4)**：通过引入“随机顺序”的多路径融合，解决了特征利用率低的问题。
4.  **最终模块 (Fig 1 right)**：所有的优化最终被集成为 **SW Conv 模块**。它使用 $3 \times 3$ 组卷积（粒度提取）和多样化的移位、重参数化、稀疏剪枝路径（多路径融合），最终以小核的代价实现了超越大核的性能。