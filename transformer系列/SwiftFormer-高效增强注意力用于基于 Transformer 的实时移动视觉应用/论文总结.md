### 1. 核心思想

该工作挑战了“大卷积核 (Large Kernel, LK) 本身是CNN性能提升的关键”这一观点。其核心思想是，大卷积核的真正优势并非其“尺寸”，而是其“效应”，这种效应可以被解耦为两个独立的组成部分：**1) 以特定粒度提取局部特征**，以及 **2) 通过多路径融合实现长距离依赖**。

为此，本文提出了一种全新的、即插即用的**Shiftwise (SW) 卷积**算子。SW Conv 仅使用标准的 $3 \times 3$ 卷积（负责粒度提取）并结合一个高效的“移位”(Shift) 操作（负责多路径融合），从而以小卷积核的成本，完美地模拟并超越了巨型卷积核的性能。这是一个纯粹的CNN架构，它证明了通过精巧的结构设计，标准 $3 \times 3$ 卷积足以重新夺回CNN在视觉任务中的SOTA地位。

### 2. 背景与动机

* **背景：大核CNN的复兴**
    * 近年来，ViT (Vision Transformer) 因其强大的长距离依赖建模能力而主导了视觉领域。
    * 为了反击，CNNs 社区借鉴了 ViT 的思想，通过极端地扩大卷积核尺寸（例如 ConvNext, RepLKNet, SLaK, UniRepLKNet 中的 $31 \times 31$ 甚至 $51 \times 51$）来强行增大感受野 (ERF)，并成功地在多项任务上“再次伟大”。

* **动机：大核的“收益递减”**
    * **问题**：研究发现，简单地增大核尺寸（例如从 $51 \times 51$ 增加到更大）带来的性能提升**收益递减**，甚至停滞。这表明“尺寸”本身可能不是根本原因，LK成功的背后隐藏着未被探索的因素。
    * **生物启发 (Figure 1)**：本文从人眼视网膜结构中获得灵感。视网膜处理视觉信号时，并非使用一个“巨型”传感器，而是采用**解耦**策略：
        1.  **粒度提取**：由大量小型的感光细胞（视杆细胞和视锥细胞）负责。
        2.  **多路径融合**：通过一个复杂的神经网络（水平细胞、双极细胞等）将信号以**多条路径**进行整合，然后传递给神经节细胞。
    * **核心假设**：现有的大核CNN（如SLaK）的成功，是无意中同时实现了这两个功能。本文的动机就是将这两个功能**显式地解耦**：
        1.  “粒度提取”是否可以用小卷积核（如 $3 \times 3$）完成？
        2.  “多路径融合”是否可以设计一个更高效的操作来实现长距离依赖？

### 3. 主要贡献点

1.  **重新定义了大卷积核 (LK) 的作用**
    * **贡献**：本文提出了一个全新的视角，即 LK 的效应被解耦为“特定粒度的特征提取”和“多路径特征融合”。这从根本上挑战了“核尺寸越大越好”的流行观点，为CNN架构设计开辟了新思路。
    * **差异**：与 SLaK、UniRepLKNet 等工作专注于“如何设计和优化 *更大* 的核”不同，本文专注于“如何 *替代* 大核”，证明了LK的效应是可模拟的。

2.  **提出 Shiftwise (SW) 卷积算子**
    * **贡献**：设计了一种简单、高效、即插即用的 **SW Conv** 模块。该模块是本文理念的核心实现。
    * **工作机理**：它使用并行的 $3 \times 3$ **组卷积 (Group Conv)** 来充当“粒度提取器”。然后，它将组卷积的多个输出通道（视为“路径”）通过一个**移位 (Shift)** 操作在空间上进行偏移和稀疏组合（模仿视网膜的多路径连接），最后将这些路径求和。
    * **差异**：它不是一个“卷积核”，而是一个“操作流”。它在功能上等同于一个巨大的、稀疏的、可分解的卷积核，但其实现完全基于高效的标准 $3 \times 3$ 卷积。

3.  **实验证明 $3 \times 3$ 卷积足以取代大核**
    * **贡献**：通过一系列严谨的“替换实验”（Ablation Study），本文证明了在SLaK等SOTA模型中，将其昂贵的 $51 \times 51$ 条带卷积替换为本文的 $3 \times 3$ SW Conv 模块，**可以达到几乎相同乃至更高的性能**（见 Table 1, #0 vs #8）。
    * **差异**：这一发现是颠覆性的。它表明现代CNN的性能提升可以不依赖于对硬件不友好的巨型卷积核，从而使CNN的主流研究重新回归到VGG所倡导的“小卷积核堆叠”的思路上。

4.  **在多任务上达到 SOTA 性能**
    * **贡献**：基于 SW Conv 构建的纯CNN架构（SW-Net），在ImageNet分类、COCO检测与分割、ADE20K语义分割等多个主流基准上，其性能全面超越了SLaK, UniRepLKNet 和 Swin Transformer 等模型。

### 4. 方法细节

本文的方法学（Architecture Design）是一个逐步演进、不断优化的过程，其核心是“替换实验”。

#### A. 关键洞察：大核 = 小组卷积 + 移位 (Figure 2)

本文的起点是分析 SLaK 中使用的 $M \times N$ 条带卷积。
* **洞察 (Figure 2b)**：一个 $M \times N$ 的条带卷积，在数学上可以**等效**于：
    1.  一个输出通道数为 $\lceil M/N \rceil$ 的 $N \times N$ **组卷积**（Group Conv, G=C）。
    2.  对这 $\lceil M/N \rceil$ 个输出通道（路径）进行**空间移位 (Shift Op)**，将它们“平铺” (layout) 到 $M$ 的空间范围内。
    3.  最后将所有移位后的特征图相加。
* **验证 (Table 1, #1)**：当他们用 $N \times N$ 组卷积 + 移位操作，去替换 SLaK 中的 $M \times N$ 大核时，模型性能（82.27%）几乎与原始SLaK（82.5%）持平。**这证明了“替换”是可行的。**

#### B. 最终的 Shiftwise (SW) Conv 模块 (Figure 1 & 3)

基于上述洞察，作者设计了最终的 SW Conv 模块：

![结构图1](https://gitee.com/ChadHui/typora-image/raw/master/cv-image/20251031144138.jpg)

1.  **粒度提取 (Group Conv)**：
    * 模块的核心是一个 $3 \times 3$ 的组卷积（即 $N=3$）。
    * 为什么是 $N=3$？实验（Table 1, #7）惊奇地发现，使用 $3 \times 3$ 作为基础粒度，其性能（81.44%）**优于**使用 $5 \times 5$（81.34%）。这证明了精细的粒度提取是至关重要的。
2.  **多路径融合 (Shift & Sum)**：
    * 组卷积的输出被分为 $K = \lceil M/3 \rceil$ 条路径（$M$ 是等效的大核边长）。
    * **移位 (Shift)**：每条路径 $k$ 被施加一个不同的空间偏移量 (offset)，从而在不增加计算量的情况下，极大地扩展了感受野。
    * **稀疏性 (Pruning)**：受视网膜启发，这些连接是稀疏的。在训练中，通过粗粒度的剪枝，一些路径（即组卷积的输出通道）会被丢弃 (pruned)，从而实现数据驱动的稀疏连接（如图 1 右侧的 `pruned` 标记）。
    * **多路径求和 (Sum)**：所有经过移位和剪枝的路径被加权求和（`BN(sum(...))`），完成一次大范围的特征融合。
3.  **性能增强**：
    * **消除冗余 (Figure 3a)**：SLaK 有两个并行的条带卷积分支。作者发现这是冗余的，他们只使用**一个**组卷积模块，但施加两种不同的移位操作（例如“正向”和“反向”），达到了同样的效果，参数减半。
    * **特征利用率 (Figure 4)**：基础的移位是可预测的。为了打破这种模式，作者引入了**随机的路径排序 (Shuffled Ordering)** 和**更多的融合边 (Edges)**，极大地提高了特征图的利用率，带来了显著的性能提升 (Table 2, #10)。
    * **重参数化 (Rep)**：为了进一步提升性能，模块中并行集成了多个 $3 \times 3$ 重参数化（Rep）分支（如图 1 右侧的 `Rep`），它们在训练时增加复杂度，在推理时可被融合进主分支。

### 5. 即插即用模块的作用、适用场景和应用

* **即插即用模块**：
    **SW Conv Block** (如图 1 右侧所示的完整结构)。

* **模块作用**：
    它是一个“大核效应模拟器”。其作用是**作为任意大卷积核（LK）的直接替代品**，或者作为任何标准 $3 \times 3$ 卷积的升级版。它在不引入实际大核（如 $51 \times 51$）的计算和内存开销的前提下，为CNN提供巨大的有效感受野 (ERF) 和强大的多路径特征融合能力。

* **适用场景和具体应用**：

    1.  **升级现有的 LK-CNNs (如 SLaK, UniRepLKNet)**：
        * **应用**：直接将这些模型中的 $31 \times 31$ 或 $51 \times 51$ 卷积核，替换为使用 $3 \times 3$ 基础核的 SW Conv 模块。
        * **效果**：如论文所示，这能维持（甚至提升）SOTA 性能，同时将模型的基础算子切换回硬件友好、高度优化的 $3 \times 3$ 卷积，可能带来推理速度的优势。

    2.  **改造经典 CNN 架构 (如 ResNet, ConvNext)**：
        * **应用**：在 ResNet 的 Bottleneck 或 ConvNext 的 Block 中，用 SW Conv 替换掉标准的 $3 \times 3$ 或 $7 \times 7$ 卷积。
        * **效果**：这可以使这些经典架构“瞬间”获得（ViT同款的）长距离依赖建模能力。原始 ResNet 的感受野增长缓慢，而 SW Conv 能使其ERF“跳跃式”增长。

    3.  **用于下游任务的骨干网络 (Backbone)**：
        * **应用**：在目标检测 (COCO)、语义分割 (ADE20K) 等需要大感受野和丰富上下文信息的任务中，使用 SW-Net 作为 FPN 的骨干网络。
        * **效果**：如 Table 4 和 Table 5 所示，SW Conv 提供的巨大 ERF 使得骨干网络能提取到更具上下文感知能力的特征，从而在检测和分割任务上超越所有对手。

    4.  **计算资源受限的平台**：
        * **应用**：虽然 SW Conv 涉及多路径和移位，但其核心计算是 $3 \times 3$ 组卷积和元素求和，这些在现代 GPU 和 NPU 上是高度优化的。相比之下，一个 $51 \times 51$ 的卷积核在很多硬件上可能无法高效执行。SW Conv 为在资源受限设备上实现大核效应提供了一条可行路径。