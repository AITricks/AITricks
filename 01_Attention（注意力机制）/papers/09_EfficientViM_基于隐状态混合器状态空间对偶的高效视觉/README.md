# Efficient ViM: 基于隐状态混合器状态空间对偶的高效视觉 Mamba

### 1. 核心思想

本文针对现有轻量级视觉模型中 Attention 机制计算成本高以及标准 Mamba 模型在视觉任务中仍存在计算冗余的问题，提出了一种名为 **Efficient ViM** 的新型架构。其核心创新在于提出了 **基于隐状态混合器的状态空间对偶（HSM-SSD）** 算法，该算法通过在压缩的隐状态空间而非原始高维序列空间中执行通道混合（Channel Mixing）和门控操作，显著降低了线性投影的计算复杂度。此外，论文还引入了单头设计以减少内存访问瓶颈，并提出了多阶段隐状态融合（MSF）策略以增强模型的特征表示能力。最终，Efficient ViM 在 ImageNet-1k 上实现了新的速度与精度的 SOTA 权衡，优于 MobileNetV3 和最新的 SHVIT 等模型。

### 2. 背景与动机

* **文本角度总结**：
    在资源受限的边缘设备上部署神经网络需要极高的效率。早期的轻量级模型（如 MobileNet）主要依赖卷积（CNN）来提取局部特征，而随后的混合架构引入了 Vision Transformer (ViT) 来捕捉全局依赖，但 Attention 的二次方复杂度 $O(L^2)$ 限制了其效率。尽管状态空间模型（SSM，如 Mamba）提供了线性的全局建模能力 $O(L)$，但现有的视觉 Mamba（如 Vim, VMamba）在推理速度上仍不如高度优化的轻量级 CNN。作者观察到，Mamba2 中引入的状态空间对偶（SSD）层，其运行时间瓶颈主要来自于对输入序列进行的线性投影操作（Linear Projections），这导致了不必要的计算开销。因此，本文旨在设计一种改进的 SSD 层，在保持全局感受野的同时，大幅削减这些线性投影的成本。

* **动机图解分析**：
    * **图 1（Figure 1）：速度-精度权衡图**
        * **现象**：该图展示了 ImageNet-1k 上各轻量级模型的 Top-1 准确率与吞吐量（Throughput）的关系。
        * **分析**：Efficient ViM（图中的红色和蓝色五角星）位于帕累托前沿的最左上方，这意味着在相同的速度下它精度最高，或在相同精度下速度最快。例如，相比于经典的 MobileNetV3，Efficient ViM 实现了 80% 的速度提升且精度更高；相比于最新的 SOTA 模型 SHVIT，也有显著优势。这直观地证明了现有方法在效率上仍有提升空间，而 Efficient ViM 成功突破了这一瓶颈。
        
    * **图 2（Figure 2）：NC-SSD 与 HSM-SSD 的复杂度对比**
        
        ![结构图2](https://gitee.com/ChadHui/typora-image/raw/master/cv-image/20251118204902.jpg)
        
        * **对比**：左图 (a) 是标准的非因果 SSD (NC-SSD)，其中的红色块代表线性层，其复杂度与序列长度 $L$ 和通道数 $D$ 相关，即 $O(LD^2)$。右图 (b) 是本文提出的 HSM-SSD。
        * **核心问题**：作者指出 NC-SSD 中主要的计算量浪费在对全长序列 $x$ 进行投影。
        * **解决方案**：HSM-SSD 将原本在 $L$ 维度进行的昂贵操作（红色块），转移到了压缩后的隐状态 $N$ 维度（橙色块），即 $O(ND^2)$。由于状态数 $N$ 远小于序列长度 $L$，这直接解决了计算效率瓶颈。
        
    * **图 3（Figure 3）：运行时分解图**
        * **现象**：该图分析了多头（Multi-Head）设计带来的内存开销。左饼图显示多头设计中，“Copy & Reshape”（橙色部分）占据了 25.2% 的时间。
        * **分析**：这揭示了理论 FLOPs 低并不代表实际推理快。多头机制引入了大量的内存读写操作（Memory-bound）。因此，本文改为单头设计（右饼图），将内存操作占比降至 5.1%，进一步提升了实际推理速度。

### 3. 主要贡献点

* **[贡献点 1]：提出了基于隐状态混合器的 SSD (HSM-SSD)**
    为了解决标准 SSD 层中线性投影成本过高的问题，作者设计了 HSM-SSD。它利用隐状态作为输入的压缩潜在表示，在隐状态空间内执行门控和线性投影操作。这一设计将核心计算复杂度从与序列长度相关 $O(LD^2)$ 降低为与状态数相关 $O(ND^2)$，在处理高分辨率图像时优势尤为明显。

* **[贡献点 2]：针对硬件效率优化的单头设计与网络架构**
    作者分析发现，传统的多头设计在轻量级模型中会带来沉重的内存访问负担（Memory-bound）。因此，Efficient ViM 采用了单头 HSM-SSD 设计，通过消除张量重塑和拷贝操作来最大化实际吞吐量，同时通过引入状态级的重要性权重来弥补去除多头带来的表征能力损失。

* **[贡献点 3]：多阶段隐状态融合 (Multi-stage Hidden State Fusion, MSF)**
    为了进一步提升模型性能，作者提出了一种融合机制，利用网络不同阶段（Stage）的隐状态来辅助最终的预测。通过对各阶段隐状态进行加权求和并参与 Logit 计算，该机制增强了隐状态的表征能力，并丰富了多尺度特征的利用，在不显著增加推理延迟的情况下提升了准确率。

### 4. 方法细节

* **整体网络架构**：
    
    ![结构图4](https://gitee.com/ChadHui/typora-image/raw/master/cv-image/20251118204920.jpg)
    
    * **宏观流程（Figure 4 左）**：
        Efficient ViM 采用了标准的分层金字塔结构。
        1.  **Stem Layer**：输入图像（$H \times W \times 3$）首先经过 Stem 层（由四个 $3 \times 3$ 卷积组成，步长为 2），将分辨率下采样为 $H/16 \times W/16$。
        2.  **Stages 1-3**：随后进入三个主要的 Stage。每个 Stage 包含堆叠的 **Efficient ViM Block**。
        3.  **Patch Merging**：在 Stage 之间，使用下采样层（Patch Merging）降低分辨率并增加通道数，构建分层特征。
        4.  **Head (MSF)**：在最后输出阶段，引入了 **多阶段隐状态融合 (MSF)** 模块，将各阶段的隐状态聚合，与最终的特征图一起用于分类预测。
    
* **核心创新模块详解（HSM-SSD Layer - Figure 2b & Algorithm 1）**：

    * **模块 A：隐状态混合器 (Hidden State Mixer, HSM)**
        * **内部结构**：这是本论文最核心的算子。不同于标准 SSD 先计算输出 $y$ 再进行线性投影，HSM-SSD 先计算隐状态 $h$。
        * **数据流**：
            1.  **输入投影与离散化**：输入 $x_{in}$ 经过轻量级线性层生成参数 $B, C, \Delta$。
            2.  **隐状态生成**：计算初始隐状态 $h_{in} = (A \odot B)^T x_{in}$。这一步将维度从 $L$（序列长度）压缩到了 $N$（状态数，通常 $N \ll L$）。
            3.  **隐状态混合 (HSM)**：在 $h_{in}$ 上进行核心的通道混合操作。具体为：$h = \text{Linear}(h_{in} \odot \sigma(\text{Linear}(h_{in})))$。这里包含了一个门控机制（$\sigma$）和线性投影。
            4.  **输出生成**：最后通过 $x_{out} = C h$ 将更新后的隐状态投影回原始序列空间。
        * **设计目的**：通过在 $N$ 维度而非 $L$ 维度进行密集的矩阵乘法，极大地减少了 FLOPs，同时利用隐状态的全局压缩特性捕捉上下文信息。

    * **模块 B：Efficient ViM Block (Figure 4 右)**
        * **内部结构**：类似于 Transformer Block，由两个子模块组成：HSM-SSD 模块和前馈网络（FFN）。
        * **数据流**：
            1.  **局部特征提取**：输入首先经过一个 $3 \times 3$ 深度卷积（DWConv），用于捕获局部空间信息（这也是轻量级模型的标准操作）。
            2.  **全局特征提取**：经过 LayerNorm 后，数据进入 **HSM-SSD** 层，负责捕捉全局依赖。
            3.  **通道交互**：最后经过另一个 $3 \times 3$ DWConv 和 FFN（由两个 $1 \times 1$ 卷积组成），进行通道间的信息交互。
            4.  **残差连接**：每个子模块都配有残差连接。
        * **设计理念**：结合卷积的局部归纳偏置和 SSM 的线性全局建模能力，同时保证推理速度。

* **理念与机制总结**：
    * **核心理念**：**“在压缩空间做昂贵运算”**。标准 Attention 或 SSD 在 Token 数量巨大的序列空间做混合，成本高昂。HSM-SSD 认为隐状态 $h$ 本身就是一种对输入的紧凑总结，因此在 $h$ 上做混合既能捕捉全局信息，又能大幅降低计算量。
    * **公式解读**：
        原 SSD 输出：$x_{out} = (Ch \odot \sigma(z)) W_{out}$ （在 $L$ 空间运算）。
        HSM-SSD 近似：$x_{out} \approx C ((h \odot \sigma(h W_z)) W_{out})$ （在 $N$ 空间运算）。
        这在数学上利用了线性变换的结合律，将计算转移到了维度更小的中间变量上。

* **图解总结**：
    结合 **Figure 2** 和 **Figure 4**，Efficient ViM 通过在 Block 内部替换掉昂贵的 NC-SSD 为 HSM-SSD，并在网络末端通过虚线连接（MSF）利用中间层的隐状态。这种设计使得数据流在主干网络中保持高效流动（由 HSM 加速），同时在最后汇聚多层级信息以保证精度，完美解决了“全局建模成本高”和“轻量级模型表征弱”的矛盾。

### 5. 即插即用模块的作用

本文提出的技术具有很好的通用性，可以作为即插即用的模块应用于其他轻量级架构设计中：

1.  **HSM-SSD 模块 (Hidden State Mixer-based SSD)**
    * **适用场景**：任何需要**线性复杂度全局建模**的轻量级视觉任务（分类、检测、分割）。
    * **具体应用**：
        * **移动端主干网络替换**：可以直接替换 MobileNet 或 EdgeViT 中的 Attention 模块或大核卷积模块，显著降低 FLOPs 和推理延迟，特别是在处理高分辨率输入（如 $512^2$ 或更大）时优势巨大。
        * **实时语义分割**：在编码器阶段使用 HSM-SSD 替代 Self-Attention，能够以极低的计算成本提供全局感受野，这对于分割任务中的上下文理解至关重要。

2.  **多阶段隐状态融合 (MSF) 策略**
    * **适用场景**：基于 SSM 或 RNN 的层级式网络架构。
    * **具体应用**：
        * **辅助监督/特征增强**：在任何基于 Mamba 或 LSTM 的视觉模型中，提取各 Stage 的隐状态 $h$，通过简单的加权平均和线性层生成 Logits 并融合到最终输出中。这是一种几乎零成本（仅增加少量参数，几乎不增加推理计算量）的涨点技巧，可增强模型的泛化能力。

3.  **单头 SSD 设计 (Single-Head Design)**
    * **适用场景**：受限于**内存带宽**的边缘设备部署。
    * **具体应用**：
        * **FPGA/移动端加速**：如果发现模型在特定硬件上的推理瓶颈在于内存读写（Memory-bound）而非计算（Compute-bound），可以将多头注意力或多头 SSM 替换为这种单头加状态级权重（State-wise Importance）的设计，以减少 Tensor 的 Reshape 和 Copy 操作。