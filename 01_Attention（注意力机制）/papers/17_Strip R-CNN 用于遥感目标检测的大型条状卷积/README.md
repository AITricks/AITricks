
# Strip R-CNN: 用于遥感目标检测的大型条状卷积

### 1. 核心思想

本文针对遥感目标检测中普遍存在的高长宽比（细长）物体检测难题，提出了一种名为 **Strip R-CNN** 的高效网络架构。该方法的核心论点是：相比于传统的方形大卷积核，**大型条状卷积（Large Strip Convolution）** 能更有效地捕捉细长物体的各向异性特征。通过在骨干网络中序贯地引入正交的大尺度条状卷积，并在检测头中解耦定位任务并使用条状模块增强，Strip R-CNN 能够以极低的参数量（30M）在 DOTA-v1.0 数据集上实现 SOTA 性能（82.75% mAP）。

### 2. 背景与动机

* **文本角度总结**：
    遥感图像中的目标往往具有任意方向和极端的长宽比（如桥梁、港口、船舶等）。现有的旋转目标检测器通常依赖于**方形大卷积核**（如 LSKNet, PKINet）来扩大感受野以捕捉长距离上下文。然而，作者认为方形卷积核在处理细长物体时会引入大量无关的背景噪声，且在计算上存在冗余。此外，传统的检测头（如 Oriented R-CNN Head）通常将分类和定位任务耦合，或仅使用全连接层/小卷积核进行定位，这限制了对细长物体进行精确定位所需的长距离依赖捕捉能力。

* **动机图解分析**：
    * **图 1 (Fig. 1): DOTA 数据集统计与现有模型性能**
        * **柱状图**：展示了 DOTA 数据集中不同长宽比（Aspect Ratio）物体的数量分布。可以看出，长宽比 > 3 的细长物体占据了相当大的比例。
        * **折线图**：展示了现有 SOTA 模型（如 LSKNet, O-RCNN 等）在不同长宽比下的检测性能（mAP）。可以明显看到，随着物体长宽比的增加，所有模型的性能都呈显著下降趋势。这直观地揭示了现有方法在处理“细长物体”时的局限性，引出了本文的动机：设计一种专门针对高长宽比物体的检测器。
    * **图 6 (Fig. 6): 空间相关性图 (Spatial Correlation Map)**
        * **左图 (Oriented R-CNN Head)**：显示了传统检测头输出特征的空间相关性。可以看到其相关性范围较小，且主要集中在局部。
        * **右图 (Strip Head)**：显示了本文提出的 Strip Head 的空间相关性。可以看到其特征图上的点与周围（尤其是长距离方向上）具有更强的相关性。这对比说明了引入条状卷积能有效捕捉长距离依赖，解决了传统检测头定位能力不足的问题。

### 3. 主要贡献点

* **[贡献点 1]：提出了 Strip R-CNN 网络架构**
    设计了一个简单、高效且强大的遥感目标检测框架。该框架不依赖复杂的注意力机制或多分支结构，而是回归到卷积设计的本源，通过创新的条状卷积设计实现了卓越的性能。

* **[贡献点 2]：设计了 StripNet 骨干网络**
    提出了基于 **Strip Block** 的骨干网络。不同于 LSKNet 和 PKINet 使用的方形大核，Strip Block 采用**序贯的正交大型条状卷积**（例如 $1 \times K$ 和 $K \times 1$）来替代大方形核。这种设计在大幅降低计算量（参数量减少，FLOPs 降低）的同时，能更精准地匹配细长物体的几何特征。

* **[贡献点 3]：提出了 Strip Head 检测头**
    重新设计了检测头，将分类、角度回归和定位任务解耦。特别是在定位分支中引入了 **Strip Module**，利用条状卷积来增强定位特征的长距离感知能力，从而提高对细长物体边界框回归的准确性。

### 4. 方法细节

* **整体网络架构**：
    请参考 **Fig. 4(c)** 和 **Fig. 5**。
    
    1.  **骨干网络 (StripNet)**：输入图像经过 Stem 层后，进入 4 个阶段的特征提取。每个阶段由堆叠的 **Strip Block** 组成。Strip Block 包含两个子模块：**Strip Module**（负责空间特征提取）和 **FFN**（负责通道混合）。
    2.  **特征金字塔 (FPN)**：提取的多尺度特征被送入 FPN 进行融合。
    3.  **检测头 (Strip Head)**：基于 Oriented R-CNN 的框架，但对 Head 进行了改造。RoI 特征被送入解耦的三个分支：分类分支、角度分支和定位分支。其中定位分支嵌入了 **Strip Module**。
    
* **核心创新模块详解**：

    * **模块 A：Strip Module (条状模块) (Fig. 4 右侧放大图)**
        
        ![结构图4](https://gitee.com/ChadHui/typora-image/raw/master/cv-image/20251120204107.jpg)
        
        * **内部结构**：这是一个用于替代标准大核卷积的组件。
        * **数据流**：
            1.  **输入 (X)**：特征图首先经过一个 $5 \times 5$ 的**小方形卷积 (Square conv)**，用于捕获局部细节。
            2.  **水平条状卷积 (H Strip Conv)**：特征图接着通过一个 $1 \times K$ 的深度卷积（实验中 $K=19$），捕获水平方向的长距离依赖。
            3.  **垂直条状卷积 (V Strip Conv)**：随后通过一个 $K \times 1$ 的深度卷积，捕获垂直方向的长距离依赖。注意，这两个条状卷积是**串联 (Sequential)** 的，而非并联。
            4.  **点卷积 (PW Conv)**：最后通过一个 $1 \times 1$ 卷积进行通道融合，生成权重图 $Y$。
            5.  **加权输出**：权重图 $Y$ 与原始输入 $X$ 进行元素级相乘（Element-wise Multiplication），得到最终输出。
        * **设计目的**：通过序贯的条状卷积，模块能够模拟出大感受野，同时相比 $K \times K$ 的大方核，参数量和计算量大幅降低，且更能适应细长物体的形状。
        
    * **模块 B：Strip Head (条状检测头) (Fig. 5 下图)**
        
        ![结构图5](https://gitee.com/ChadHui/typora-image/raw/master/cv-image/20251120204126.jpg)
        
        * **内部结构**：解耦的检测头设计。
        * **数据流**：
            * **分类与角度分支**：共享两个全连接层 (FC)，然后分叉。这是基于图 7 的观察：分类和角度的敏感区域有重叠。
            * **定位分支**：RoI 特征先经过一个卷积层，然后进入核心的 **Strip Module**（同上所述），最后通过 FC 层输出位置偏移。
        * **设计理念**：传统的 FC 层缺乏空间敏感性。在定位分支引入 Strip Module，可以显式地增强特征图在长宽方向上的感知能力，这对回归细长物体的边界框（Bounding Box）至关重要。

* **理念与机制总结**：
    * **核心理念**：**“因形制宜”**。遥感物体多为细长型，且方向任意。方形卷积核会引入过多背景噪声。条状卷积（Strip Conv）就像一把尺子，可以沿着物体的长轴方向进行特征聚合，既匹配了物体的几何特性，又节省了计算资源。
    * **序贯正交机制**：通过串联 $1 \times K$ 和 $K \times 1$ 卷积，网络实际上拥有了 $K \times K$ 的理论感受野，但其关注点更侧重于十字形的轴向特征，这正好符合细长物体在旋转后的投影特性。

* **图解总结**：
    
    * **Fig. 3** 直观对比了 LSKNet（大方核）、PKINet（多尺度方核并联）和 Strip R-CNN（条状核串联）的结构。Strip R-CNN 的结构最为精简。
    * **Fig. 8** 展示了不同核大小的特征响应。可以看到 $K=19$ 时，细长物体（如桥梁）的响应最强且背景噪声最少。
    * **Fig. 9** 的 Eigen-CAM 可视化显示，相比其他方法，Strip R-CNN 的热力图更精准地覆盖了细长物体的整体，而不仅仅是局部。

### 5. 即插即用模块的作用

论文提出的 **Strip Module** 是一个极佳的即插即用模块：

1.  **Strip Module (条状模块)**
    * **适用场景**：任何涉及**细长物体检测**、**旋转目标检测**或需要**轻量化大感受野**的视觉任务。
    * **具体应用**：
        * **替换骨干网络卷积**：在 ResNet, MobileNet 或 ConvNeXt 等网络中，可以用 Strip Module 替换原本的 $3 \times 3$ 或 $7 \times 7$ 卷积块（Spatial Mixing 层）。这将显著扩大有效感受野，同时降低参数量，特别适合遥感图像、场景文字检测（Scene Text Detection）或车道线检测等任务。
        * **增强检测头 (Head)**：在 YOLO, Faster R-CNN 或 RetinaNet 的检测头（尤其是 Regression/Localization 分支）中插入 Strip Module。如论文中 **Strip Head** 所示，这能显著提升对高长宽比物体的定位精度。
        * **注意力机制替代**：Strip Module 输出的权重图与输入相乘的机制，本质上是一种**空间注意力 (Spatial Attention)**。它可以作为一种高效的 Attention 模块插入到网络的任何特征融合阶段（如 FPN 的融合后）。