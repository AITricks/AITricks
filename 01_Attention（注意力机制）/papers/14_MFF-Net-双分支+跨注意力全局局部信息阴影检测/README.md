# Multi-Level Feature Fusion Network for Shadow Removal Detection (MFF-Net) - 论文总结

### 1. 核心思想

本文针对由于阴影移除操作留下的痕迹过于微弱且难以定位的问题，提出了一种名为多级特征融合网络（MFF-Net）的新型检测模型。该模型采用了基于Transformer的双分支编码器结构，分别利用PVT（Pyramid Vision Transformer）和CMT（CNN meet Vision Transformer）来同时捕获全局长程依赖和局部结构细节。配合一个包含多尺度特征上采样（MSFU）模块的密集预测解码器，模型能够以“由粗到细”的方式逐步恢复阴影掩膜的空间细节。实验表明，MFF-Net在定位阴影移除区域的准确性上显著优于现有的通用图像篡改检测方法。

### 2. 背景与动机

* **文本背景总结**：
    随着阴影移除技术的发展，攻击者可以轻易移除图像中与伪造物体不一致的阴影，从而掩盖篡改痕迹。然而，现有的通用图像篡改检测方法（如用于检测拼接、修复或复制移动的方法）主要关注明显的纹理差异或边缘伪影。由于阴影移除主要引起亮度变化而非纹理突变，且留下的痕迹极其微弱，导致现有方法在检测此类篡改时存在严重的“语义鸿沟”，无法准确区分背景与微弱的阴影移除伪影。此外，现有模型往往缺乏针对此任务的全局建模与局部特征提取的平衡能力。

* **动机图解分析（基于 Figure 1）**：
    
    ![结构图1](https://gitee.com/ChadHui/typora-image/raw/master/cv-image/20251127202316.jpg)
    
    * **看图说话**：Figure 1 展示了一个典型的取证场景。图(a)是原始图像（含阴影），图(b)是经过篡改的图像（移除了原物体，插入了新物体，并移除了原阴影），图(c)是检测结果。
    * **图解逻辑**：
        1.  **操作隐蔽性**：从(a)到(b)，攻击者为了使插入的物体（如柯基犬或网球选手）看起来逼真，必须移除原始物体留下的不匹配阴影。这种操作在视觉上非常自然，仅留下了极微小的亮度变化痕迹。
        2.  **现有局限与核心问题**：传统的篡改检测器通常寻找拼接边界或显著的噪声不一致，但在阴影移除区域（图c中白色高亮区域），这些特征非常稀疏。这直观地展示了现有方法的**检测盲区**。
        3.  **引出本文目标**：通过展示仅通过检测“阴影移除区域”就能判定图像被篡改（即使物体本身的篡改痕迹被掩盖），该图论证了开发专用阴影移除检测器的必要性，即需要一个能感知微弱亮度变化并结合全局光照一致性的模型。

### 3. 主要创新点

* **1：双分支Transformer特征提取编码器**
    提出了一种独特的双分支架构，并首次探索了将两个具有不同信息处理能力的Transformer基模型并行作为骨干网络。与常见的CNN-Transformer混合结构不同，本文采用 **PVT（Global Modeling Branch）** 捕获全局长程依赖，采用 **CMT（Local Feature Extraction Branch）** 增强局部结构信息。由于两者均为Transformer架构，特征在融合时具有更好的兼容性，有效解决了单一网络难以兼顾全局光照一致性和局部微弱痕迹的问题。

* **2：多尺度特征上采样模块（MSFU）**
    在解码器中设计了MSFU模块来指导上采样和融合过程。不同于简单的双线性插值，该模块引入了类似于“Fold”操作的机制来扩展Token，在保持空间结构的同时进行上采样。它通过交叉注意力机制（Cross Attention）将编码器中的低级特征与解码器的高级特征进行深度融合，有效防止了上采样过程中的信息丢失。

* **3：由粗到细的密集预测策略**
    采用了一种密集预测解码结构，通过多级监督策略（Multi-level Supervision）在不同分辨率下估计阴影掩膜。解码器利用深层特征锚定篡改区域的大致位置，并逐级利用浅层特征细化边界细节，最终实现了高精度的像素级定位。

### 4. 方法细节

#### 整体网络架构（Data Flow）

请参考 **Figure 2 (Architecture of MFF-Net)**：

![结构图2](https://gitee.com/ChadHui/typora-image/raw/master/cv-image/20251127202341.jpg)

1.  **输入 (Input)**：输入一张经过预处理的RGB图像（尺寸调整为 $224 \times 224$）。
2.  **编码阶段 (Encoder Path)**：
    * 输入图像被同时送入两个并行分支：
        * **全局建模分支 (上路)**：使用 PVT-v2-b2 骨干网。图像经过 Patch Embedding 后进入 Transformer 块，产生三个阶段的特征 $T_1, T_2, T_3$（分辨率依次降低：$56^2, 28^2, 14^2$）。此路径负责建立长程依赖。
        * **局部特征提取分支 (下路)**：使用 CMT-S 骨干网。同样经过三个阶段，生成特征 $T'_1, T'_2, T'_3$。此路径引入了卷积层，专注于提取局部纹理和结构。
    * **特征融合**：两个分支对应尺度的特征首先进行融合（Concatenation + Linear + Gelu），得到融合后的编码器特征 $T^E_i$。此外，浅层特征还经过了 DSWSAM 模块进行局部增强。
3.  **解码阶段 (Decoder Path)**：
    * 最高层的融合特征首先通过一个 Transformer Layer 进行处理。
    * 解码器包含三个级联的阶段。数据流从低分辨率向高分辨率流动。
    * 在每个阶段，特征进入 **MSFU模块** 进行上采样，并与编码器传来的低级特征进行融合。
    * 随后经过交叉注意力（Cross Attention）模块进一步细化。
4.  **输出 (Output)**：
    * 在解码器的每一级（共4级，$14\times14$ 到 $112\times112$等），都会通过线性投影层输出一个预测掩膜（$\hat{M}_4, \hat{M}_3, \hat{M}_2, \hat{M}_1$）。
    * 计算损失时，同时考虑二元交叉熵损失（$L_{bce}$）和IoU损失（$L_{iou}$），对所有尺度的输出进行加权监督。

#### 核心创新模块详解

* **模块 A：多尺度特征上采样模块 (MSFU)**
    
    ![结构图3](https://gitee.com/ChadHui/typora-image/raw/master/cv-image/20251127202401.jpg)
    
    * **设计目的**：解决传统上采样丢失细节的问题，并有效融合编码器的低级特征。
    * **内部结构拆解**：
        1.  **特征上采样 (FU)**：输入特征 $X$ 首先经过线性变换扩展通道数，然后通过 `Fold` 操作将通道维度的数据重排到空间维度，实现分辨率提升（类似PixelShuffle）。
        2.  **局部增强**：上采样后的特征经过深度卷积（DWConv）提取局部信息，并与原特征残差连接。
        3.  **交叉注意力融合**：
            * **Query ($Q_1$)**：来自解码器当前经过上采样的特征。
            * **Key ($K_2$) & Value ($V_2$)**：来自编码器同尺度的低级特征 $T^D$。
            * 通过 $Attention(Q_1, K_2, V_2)$，解码器特征能够从编码器特征中动态“查询”并聚合所需的纹理细节信息。
    * **数据流向**：低分率特征 $\rightarrow$ Fold上采样 $\rightarrow$ DWConv $\rightarrow$ Cross Attention (与Skip Connection特征交互) $\rightarrow$ 输出高分率特征。
    
* **[Figure 4] 模块 B：基于注意力的前馈模块 (AFF)**
    
    ![结构图4](https://gitee.com/ChadHui/typora-image/raw/master/cv-image/20251127202417.jpg)
    
    * **设计目的**：在解码器中进一步提取全局和局部结构信息，替代传统的MLP层以提升效率和性能。
    * **内部结构拆解**：
        1.  **轻量级多头自注意力 (LMHSA)**：输入特征 $X$ 生成 $Q, K, V$。为了降低计算量，$K$ 和 $V$ 在计算注意力图之前先经过 $k \times k$ 的深度卷积进行空间降维。
        2.  **局部卷积**：在自注意力分支之后，并联一个分组卷积（Group Conv）分支来补充局部归纳偏置。
        3.  **残差连接**：$Y = LMHSA(X) + X$，随后 $Z = Conv(Y) + Y$。
    * **机制**：通过在解码器深层引入自注意力，模型能够在恢复空间分辨率的同时，保持对全局上下文的感知，确认阴影移除区域的整体一致性。

#### 理念与机制总结

* **双Transformer互补机制**：
    论文的核心理念是认为 CNN 与 Transformer 的特征在底层分布上存在差异，直接融合（Hybrid架构）效果不佳。因此，MFF-Net 采用了“同质异构”的策略——两个分支都是 Transformer，但分别侧重 Global（PVT）和 Local（CMT）。PVT 通过金字塔结构捕捉光照分布的一致性，CMT 通过引入卷积捕捉微弱的边缘痕迹。
* **由粗到细的监督机制**：
    公式 $L_G = \sum_{i=1}^4 \lambda_i (L_{bce} + L_{iou})$ 体现了其工作机制。$\lambda_i$ 随分辨率提高而减小，意味着模型首先强迫网络在低分辨率下学会“找对位置”（Anchor），然后在高分辨率下专注于“描绘边缘”（Refine），从而解决了阴影移除痕迹模糊难寻的问题。

#### 图解总结

针对“动机图解”中提出的“微弱痕迹难以定位”的问题，MFF-Net 的设计实现了精确打击：
1.  **PVT分支** 解决了图1中因光照变化引起的语义不一致问题，因为它能看到全局上下文。
2.  **CMT分支** 捕捉到了肉眼难以察觉的局部像素级伪影。
3.  **MSFU模块** 确保了在还原掩膜大小（从图1(c)的缩略图到全图）的过程中，不会因为上采样模糊而丢失那些本就微弱的边界信息。

### 5. 即插即用模块的作用

以下模块具有高度的可复用性，适用于其他计算机视觉任务：

1.  **MSFU (Multi-Scale Feature Upsampling) 模块**：
    * **适用场景**：语义分割、显著性目标检测、医学图像分割。
    * **作用**：替代传统的 `Upsample` 或 `Deconv` 层。特别适用于那些需要从低分辨率特征中恢复高频细节，且存在跳跃连接（Skip Connection）架构的网络（如U-Net变体）。

2.  **AFF (Attention Based Feed Forward) 模块**：
    * **适用场景**：移动端或轻量级视觉Transformer设计。
    * **作用**：可以作为标准 Transformer Block 中 FFN（前馈神经网络）的替代品。它在保持较低参数量和计算量（MACs）的同时，增强了特征的局部提取能力，适合资源受限的边缘计算场景。

3.  **双分支Transformer编码策略 (Dual-Branch Strategy)**：
    * **适用场景**：Deepfake 检测、隐写分析、去雨/去雾检测。
    * **作用**：对于那些既需要判别全局统计特征（如光照、噪声分布），又需要定位局部细微篡改痕迹的任务，这种分离关注点再融合的策略非常有效。