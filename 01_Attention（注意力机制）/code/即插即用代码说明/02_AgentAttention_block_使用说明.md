# Agent Attention å³æ’å³ç”¨æ¨¡å—ä½¿ç”¨è¯´æ˜

## ğŸ“– æ¨¡å—ç®€ä»‹

Agent Attention æ˜¯ä¸€ä¸ªç”¨äº Stable Diffusion æ¨¡å‹çš„å³æ’å³ç”¨åŠ é€Ÿæ¨¡å—ï¼Œé€šè¿‡ç»“åˆ **ToMe (Token Merging)** å’Œ **Agent Attention** æœºåˆ¶ï¼Œå¯ä»¥åœ¨ä¸é‡æ–°è®­ç»ƒæ¨¡å‹çš„æƒ…å†µä¸‹ï¼š

- âœ… **åŠ é€Ÿæ¨ç†é€Ÿåº¦**ï¼šå‡å°‘è®¡ç®—é‡ï¼Œæå‡ç”Ÿæˆé€Ÿåº¦
- âœ… **é™ä½å†…å­˜å ç”¨**ï¼šå‡å°‘ GPU å†…å­˜ä½¿ç”¨
- âœ… **æå‡å›¾åƒè´¨é‡**ï¼šåœ¨åŠ é€Ÿçš„åŒæ—¶æå‡ç”Ÿæˆå›¾åƒçš„è´¨é‡
- âœ… **æ— éœ€é‡è®­ç»ƒ**ï¼šç›´æ¥åº”ç”¨åˆ°ç°æœ‰æ¨¡å‹ï¼Œæ— éœ€é¢å¤–è®­ç»ƒ

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒè¦æ±‚

```bash
# å¿…éœ€çš„ä¾èµ–åŒ…
torch >= 1.12.1
einops
```

### 2. å®‰è£…

å°† `AgentAttention_block.py` æ–‡ä»¶æ”¾ç½®åœ¨æ‚¨çš„é¡¹ç›®ç›®å½•ä¸­ï¼Œç¡®ä¿ `agentsd` ç›®å½•ï¼ˆåŒ…å« `merge.py` å’Œ `utils.py`ï¼‰åœ¨åŒä¸€ç›®å½•ä¸‹ã€‚

```
é¡¹ç›®ç›®å½•/
â”œâ”€â”€ AgentAttention_block.py
â”œâ”€â”€ agentsd/
â”‚   â”œâ”€â”€ merge.py
â”‚   â”œâ”€â”€ utils.py
â”‚   â””â”€â”€ __init__.py
â””â”€â”€ æ‚¨çš„ä»£ç .py
```

### 3. æµ‹è¯•æ¨¡å—

è¿è¡Œæµ‹è¯•è„šæœ¬éªŒè¯æ¨¡å—æ˜¯å¦æ­£å¸¸å·¥ä½œï¼š

```bash
python AgentAttention_block.py
```

å¦‚æœçœ‹åˆ°æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Œè¯´æ˜æ¨¡å—å·²æ­£ç¡®å®‰è£…ã€‚

## ğŸ’¡ åŸºæœ¬ä½¿ç”¨

### æ–¹æ³•ä¸€ï¼šç›´æ¥å¯¼å…¥ä½¿ç”¨

```python
from AgentAttention_block import apply_patch, remove_patch

# åŠ è½½æ‚¨çš„ Stable Diffusion æ¨¡å‹
# model = ... æ‚¨çš„æ¨¡å‹åŠ è½½ä»£ç 

# åº”ç”¨ Agent Attention è¡¥ä¸
apply_patch(
    model,
    ratio=0.4,           # tokenåˆå¹¶æ¯”ä¾‹
    agent_ratio=0.8,     # agent tokenæ¯”ä¾‹
    k_scale2=0.3,        # ç¬¬äºŒé˜¶æ®µæ³¨æ„åŠ›ç¼©æ”¾å› å­
    k_shortcut=0.075     # æ®‹å·®è¿æ¥ç³»æ•°
)

# ä½¿ç”¨æ¨¡å‹è¿›è¡Œæ¨ç†
# ... æ‚¨çš„æ¨ç†ä»£ç  ...

# å¯é€‰ï¼šç§»é™¤è¡¥ä¸ï¼Œæ¢å¤åŸå§‹æ¨¡å‹
remove_patch(model)
```

### æ–¹æ³•äºŒï¼šåœ¨æ‰©æ•£è¿‡ç¨‹ä¸­åŠ¨æ€åº”ç”¨

```python
from AgentAttention_block import apply_patch, remove_patch

# åœ¨æ‰©æ•£è¿‡ç¨‹çš„ä¸åŒé˜¶æ®µä½¿ç”¨ä¸åŒå‚æ•°
for step in range(num_steps):
    if step == 0:
        # æ—©æœŸæ­¥éª¤ï¼šä½¿ç”¨è¾ƒå¼ºçš„tokenåˆå¹¶
        apply_patch(model, ratio=0.4, agent_ratio=0.95, sx=4, sy=4)
    elif step == 20:
        # åæœŸæ­¥éª¤ï¼šä½¿ç”¨è¾ƒå¼±çš„tokenåˆå¹¶
        remove_patch(model)
        apply_patch(model, ratio=0.4, agent_ratio=0.5, sx=2, sy=2)
    
    # æ‰§è¡Œæ‰©æ•£æ­¥éª¤
    # ... æ‚¨çš„æ‰©æ•£ä»£ç  ...
```

## ğŸ“š è¯¦ç»†å‚æ•°è¯´æ˜

### `apply_patch()` å‡½æ•°å‚æ•°

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `model` | `torch.nn.Module` | - | Stable Diffusion æ¨¡å‹å¯¹è±¡ï¼ˆå¿…éœ€ï¼‰ |
| `ratio` | `float` | `0.5` | Tokenåˆå¹¶æ¯”ä¾‹ï¼ŒèŒƒå›´ [0, 1]ï¼Œå€¼è¶Šå¤§åˆå¹¶è¶Šå¤š |
| `max_downsample` | `int` | `1` | åº”ç”¨è¡¥ä¸çš„æœ€å¤§ä¸‹é‡‡æ ·å±‚æ•°ï¼ˆ1, 2, 4, 8ï¼‰ |
| `sx`, `sy` | `int` | `2` | Tokenåˆå¹¶çš„strideï¼ˆæ­¥é•¿ï¼‰ |
| `agent_ratio` | `float` | `0.8` | Agent tokenç”Ÿæˆæ—¶çš„åˆå¹¶æ¯”ä¾‹ |
| `k_scale2` | `float` | `0.3` | Agent Attentionç¬¬äºŒé˜¶æ®µæ³¨æ„åŠ›çš„ç¼©æ”¾å› å­ |
| `k_shortcut` | `float` | `0.075` | æ®‹å·®è¿æ¥ç³»æ•° |
| `attn_precision` | `str` | `None` | æ³¨æ„åŠ›è®¡ç®—ç²¾åº¦ï¼Œ`"fp32"` å¯é¿å…æ•°å€¼ä¸ç¨³å®šï¼ˆSD v2.1æ¨èï¼‰ |
| `use_rand` | `bool` | `True` | æ˜¯å¦ä½¿ç”¨éšæœºæ‰°åŠ¨ |
| `merge_attn` | `bool` | `True` | æ˜¯å¦åœ¨è‡ªæ³¨æ„åŠ›å±‚åˆå¹¶tokensï¼ˆæ¨èï¼‰ |
| `merge_crossattn` | `bool` | `False` | æ˜¯å¦åœ¨äº¤å‰æ³¨æ„åŠ›å±‚åˆå¹¶tokensï¼ˆä¸æ¨èï¼‰ |
| `merge_mlp` | `bool` | `False` | æ˜¯å¦åœ¨MLPå±‚åˆå¹¶tokensï¼ˆä¸æ¨èï¼‰ |

### `remove_patch()` å‡½æ•°

```python
remove_patch(model)
```

ç§»é™¤ Agent Attention è¡¥ä¸ï¼Œæ¢å¤åŸå§‹æ¨¡å‹ã€‚ä¼šæ¸…é™¤æ‰€æœ‰hookså¹¶å°†æ¨¡å—ç±»æ¢å¤ä¸ºåŸå§‹ç±»ã€‚

## ğŸ¯ æ¨èé…ç½®

### Stable Diffusion v1.5

```python
apply_patch(
    model,
    ratio=0.4,
    agent_ratio=0.8,
    k_scale2=0.3,
    k_shortcut=0.075,
    max_downsample=1,
    sx=2,
    sy=2
)
```

### Stable Diffusion v2.1

```python
apply_patch(
    model,
    ratio=0.4,
    agent_ratio=0.8,
    k_scale2=0.3,
    k_shortcut=0.075,
    attn_precision="fp32",  # é‡è¦ï¼šé¿å…æ•°å€¼ä¸ç¨³å®š
    max_downsample=1,
    sx=2,
    sy=2
)
```

### é«˜åˆ†è¾¨ç‡ç”Ÿæˆï¼ˆ512x512åŠä»¥ä¸Šï¼‰

```python
apply_patch(
    model,
    ratio=0.3,          # é™ä½åˆå¹¶æ¯”ä¾‹ä»¥ä¿æŒè´¨é‡
    agent_ratio=0.7,
    max_downsample=2,   # å…è®¸æ›´å¤šå±‚åº”ç”¨è¡¥ä¸
    sx=4,
    sy=4
)
```

## ğŸ“ å®Œæ•´ç¤ºä¾‹

### ç¤ºä¾‹1ï¼šåŸºæœ¬ä½¿ç”¨ï¼ˆStable Diffusion v1.5ï¼‰

```python
import torch
from diffusers import StableDiffusionPipeline
from AgentAttention_block import apply_patch, remove_patch

# åŠ è½½æ¨¡å‹
device = "cuda" if torch.cuda.is_available() else "cpu"
pipe = StableDiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    torch_dtype=torch.float16
).to(device)

# åº”ç”¨ Agent Attention è¡¥ä¸
apply_patch(pipe, ratio=0.4, agent_ratio=0.8)

# ç”Ÿæˆå›¾åƒ
prompt = "a beautiful landscape"
image = pipe(prompt).images[0]
image.save("output.png")

# å¯é€‰ï¼šç§»é™¤è¡¥ä¸
remove_patch(pipe)
```

### ç¤ºä¾‹2ï¼šåœ¨æ‰©æ•£è¿‡ç¨‹ä¸­åŠ¨æ€åº”ç”¨

```python
from AgentAttention_block import apply_patch, remove_patch

def custom_sampling_loop(model, prompt, num_inference_steps=50):
    # åˆå§‹åŒ–
    latents = ...
    
    for i, t in enumerate(timesteps):
        # åœ¨æ—©æœŸæ­¥éª¤åº”ç”¨å¼ºåˆå¹¶
        if i == 0:
            remove_patch(model)  # å…ˆç§»é™¤ä¹‹å‰çš„è¡¥ä¸
            apply_patch(model, ratio=0.4, agent_ratio=0.95, sx=4, sy=4)
        
        # åœ¨ä¸­æœŸæ­¥éª¤è°ƒæ•´å‚æ•°
        elif i == num_inference_steps // 2:
            remove_patch(model)
            apply_patch(model, ratio=0.4, agent_ratio=0.7, sx=2, sy=2)
        
        # æ‰§è¡Œæ‰©æ•£æ­¥éª¤
        noise_pred = model(latents, t, prompt)
        latents = scheduler.step(noise_pred, t, latents).prev_sample
    
    return latents
```

### ç¤ºä¾‹3ï¼šæ€§èƒ½å¯¹æ¯”æµ‹è¯•

```python
import time
from AgentAttention_block import apply_patch, remove_patch

# æµ‹è¯•åŸå§‹æ¨¡å‹
start_time = time.time()
for _ in range(10):
    image = pipe(prompt).images[0]
original_time = (time.time() - start_time) / 10

# åº”ç”¨è¡¥ä¸
apply_patch(pipe, ratio=0.4, agent_ratio=0.8)

# æµ‹è¯•åŠ é€Ÿåæ¨¡å‹
start_time = time.time()
for _ in range(10):
    image = pipe(prompt).images[0]
accelerated_time = (time.time() - start_time) / 10

print(f"åŸå§‹æ¨¡å‹: {original_time:.2f}s/å¼ ")
print(f"åŠ é€Ÿåæ¨¡å‹: {accelerated_time:.2f}s/å¼ ")
print(f"åŠ é€Ÿæ¯”: {original_time/accelerated_time:.2f}x")
```

## âš™ï¸ å‚æ•°è°ƒä¼˜æŒ‡å—

### ratio (Tokenåˆå¹¶æ¯”ä¾‹)

- **0.2-0.3**ï¼šä¿å®ˆè®¾ç½®ï¼Œè´¨é‡ä¼˜å…ˆï¼Œé€Ÿåº¦æå‡è¾ƒå°
- **0.4-0.5**ï¼šå¹³è¡¡è®¾ç½®ï¼Œæ¨èä½¿ç”¨ï¼ˆ**é»˜è®¤æ¨è**ï¼‰
- **0.6-0.7**ï¼šæ¿€è¿›è®¾ç½®ï¼Œé€Ÿåº¦ä¼˜å…ˆï¼Œå¯èƒ½å½±å“è´¨é‡

### agent_ratio (Agent tokenæ¯”ä¾‹)

- **0.7-0.8**ï¼šæ ‡å‡†è®¾ç½®ï¼Œæ¨èä½¿ç”¨ï¼ˆ**é»˜è®¤æ¨è**ï¼‰
- **0.9-0.95**ï¼šæ—©æœŸæ­¥éª¤ä½¿ç”¨ï¼Œæ›´å¼ºå‹ç¼©
- **0.5-0.6**ï¼šåæœŸæ­¥éª¤ä½¿ç”¨ï¼Œä¿æŒç»†èŠ‚

### max_downsample (æœ€å¤§ä¸‹é‡‡æ ·å±‚æ•°)

- **1**ï¼šä»…åœ¨æ— ä¸‹é‡‡æ ·å±‚åº”ç”¨ï¼ˆæ¨èï¼Œè´¨é‡æœ€å¥½ï¼‰
- **2**ï¼šå…è®¸2å€ä¸‹é‡‡æ ·å±‚åº”ç”¨ï¼ˆå¹³è¡¡ï¼‰
- **4-8**ï¼šåœ¨æ‰€æœ‰å±‚åº”ç”¨ï¼ˆé€Ÿåº¦æœ€å¿«ï¼Œå¯èƒ½å½±å“è´¨é‡ï¼‰

### attn_precision (æ³¨æ„åŠ›ç²¾åº¦)

- **None**ï¼šä½¿ç”¨æ¨¡å‹é»˜è®¤ç²¾åº¦ï¼ˆSD v1.5ï¼‰
- **"fp32"**ï¼šä½¿ç”¨FP32ç²¾åº¦ï¼ˆ**SD v2.1æ¨è**ï¼Œé¿å…æ•°å€¼ä¸ç¨³å®šï¼‰

## ğŸ” å¸¸è§é—®é¢˜

### Q1: æ¨¡å—å¯¼å…¥å¤±è´¥æ€ä¹ˆåŠï¼Ÿ

**A:** ç¡®ä¿ `agentsd` ç›®å½•å­˜åœ¨ä¸”åŒ…å« `merge.py` å’Œ `utils.py` æ–‡ä»¶ã€‚å¦‚æœè¿˜æ˜¯å¤±è´¥ï¼Œæ£€æŸ¥ Python è·¯å¾„è®¾ç½®ã€‚

```python
import sys
import os
sys.path.insert(0, os.path.dirname(__file__))
```

### Q2: åº”ç”¨è¡¥ä¸åæ¨¡å‹è¾“å‡ºå¼‚å¸¸ï¼Ÿ

**A:** å°è¯•ä»¥ä¸‹æ–¹æ³•ï¼š
1. æ£€æŸ¥å‚æ•°è®¾ç½®æ˜¯å¦åˆç†ï¼ˆratioä¸è¦è¿‡å¤§ï¼‰
2. å¯¹äº SD v2.1ï¼Œè®¾ç½® `attn_precision="fp32"`
3. å°è¯•é™ä½ `max_downsample` å€¼
4. ç¡®ä¿åœ¨åº”ç”¨è¡¥ä¸å‰æ¨¡å‹å·²æ­£ç¡®åŠ è½½

### Q3: å¦‚ä½•åœ¨ä¸åŒé˜¶æ®µä½¿ç”¨ä¸åŒå‚æ•°ï¼Ÿ

**A:** åœ¨æ‰©æ•£å¾ªç¯ä¸­ä½¿ç”¨ `remove_patch()` å’Œ `apply_patch()` åŠ¨æ€åˆ‡æ¢å‚æ•°ï¼š

```python
if step == 0:
    remove_patch(model)
    apply_patch(model, ratio=0.4, agent_ratio=0.95)
elif step == 20:
    remove_patch(model)
    apply_patch(model, ratio=0.4, agent_ratio=0.5)
```

### Q4: å†…å­˜ä½¿ç”¨æ²¡æœ‰æ˜æ˜¾å‡å°‘ï¼Ÿ

**A:** 
1. å¢åŠ  `ratio` å€¼ï¼ˆä½†ä¸è¦è¶…è¿‡0.6ï¼‰
2. å¢åŠ  `max_downsample` å€¼
3. ç¡®ä¿ `merge_attn=True`ï¼ˆé»˜è®¤å·²å¼€å¯ï¼‰

### Q5: ç”Ÿæˆé€Ÿåº¦æ²¡æœ‰æ˜æ˜¾æå‡ï¼Ÿ

**A:**
1. æ£€æŸ¥æ˜¯å¦æ­£ç¡®åº”ç”¨äº†è¡¥ä¸
2. å¢åŠ  `ratio` å’Œ `agent_ratio` å€¼
3. ä½¿ç”¨æ›´å¤§çš„ `sx` å’Œ `sy` å€¼
4. ç¡®ä¿åœ¨GPUä¸Šè¿è¡Œ

### Q6: æ”¯æŒå“ªäº›æ¨¡å‹æ ¼å¼ï¼Ÿ

**A:** 
- âœ… Stable Diffusion v1.5 (LDMæ ¼å¼)
- âœ… Stable Diffusion v2.0/v2.1 (LDMæ ¼å¼)
- âœ… Diffusersåº“çš„Stable Diffusionæ¨¡å‹
- âœ… å…¶ä»–åŸºäºTransformer Blockçš„æ‰©æ•£æ¨¡å‹

## ğŸ“Š æ€§èƒ½å‚è€ƒ

æ ¹æ®è®ºæ–‡å’Œæµ‹è¯•ï¼Œä½¿ç”¨æ¨èå‚æ•°ï¼ˆratio=0.4, agent_ratio=0.8ï¼‰åœ¨ Stable Diffusion v1.5 ä¸Šï¼š

- **é€Ÿåº¦æå‡**: 1.3-1.7x åŠ é€Ÿ
- **å†…å­˜å‡å°‘**: 1.5-2.0x å‡å°‘
- **è´¨é‡æå‡**: FIDåˆ†æ•°é™ä½ 0.7-1.0ï¼ˆæ›´å¥½çš„è´¨é‡ï¼‰

## ğŸ› ï¸ é«˜çº§ç”¨æ³•

### è‡ªå®šä¹‰åˆå¹¶ç­–ç•¥

```python
# åªåœ¨è‡ªæ³¨æ„åŠ›å±‚åˆå¹¶ï¼ˆæ¨èï¼‰
apply_patch(model, merge_attn=True, merge_crossattn=False, merge_mlp=False)

# åœ¨è‡ªæ³¨æ„åŠ›å’Œäº¤å‰æ³¨æ„åŠ›å±‚éƒ½åˆå¹¶
apply_patch(model, merge_attn=True, merge_crossattn=True, merge_mlp=False)

# åœ¨æ‰€æœ‰å±‚åˆå¹¶ï¼ˆä¸æ¨èï¼Œå¯èƒ½å½±å“è´¨é‡ï¼‰
apply_patch(model, merge_attn=True, merge_crossattn=True, merge_mlp=True)
```

### æ£€æŸ¥è¡¥ä¸çŠ¶æ€

```python
# æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²åº”ç”¨è¡¥ä¸
def is_patched(model):
    for _, module in model.named_modules():
        if hasattr(module, "_tome_info"):
            return True
    return False

if is_patched(model):
    print("æ¨¡å‹å·²åº”ç”¨Agent Attentionè¡¥ä¸")
else:
    print("æ¨¡å‹æœªåº”ç”¨è¡¥ä¸")
```

## ğŸ“– å‚è€ƒèµ„æ–™

- è®ºæ–‡: [Agent Attention: On the Integration of Softmax and Linear Attention](https://arxiv.org/abs/2312.08874)
- åŸå§‹é¡¹ç›®: [Agent-Attention](https://github.com/...)
- ToMeSD: [Token Merging for Stable Diffusion](https://github.com/dbolya/tomesd)

## ğŸ“„ è®¸å¯è¯

è¯·å‚è€ƒåŸå§‹é¡¹ç›®çš„è®¸å¯è¯æ–‡ä»¶ã€‚

## ğŸ¤ è´¡çŒ®

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œæ¬¢è¿æäº¤Issueæˆ–Pull Requestã€‚

---

**ç¥ä½¿ç”¨æ„‰å¿«ï¼å¦‚æœ‰ä»»ä½•é—®é¢˜ï¼Œè¯·å‚è€ƒå¸¸è§é—®é¢˜éƒ¨åˆ†æˆ–æŸ¥çœ‹ä»£ç æ³¨é‡Šã€‚**

