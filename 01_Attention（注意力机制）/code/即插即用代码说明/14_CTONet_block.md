# CTOæ¶æ„å³æ’å³ç”¨æ¨¡å—è¯´æ˜æ–‡æ¡£

## ğŸ“‹ ç›®å½•

- [æ¦‚è¿°](#æ¦‚è¿°)
- [æ¨¡å—åˆ—è¡¨](#æ¨¡å—åˆ—è¡¨)
- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
- [æ¨¡å—è¯¦ç»†è¯´æ˜](#æ¨¡å—è¯¦ç»†è¯´æ˜)
  - [Res2Net Bottle2neck](#1-res2net-bottle2neck)
  - [Stitch Attention](#2-stitch-attention)
  - [Position Attention Module](#3-position-attention-module)
  - [Channel Attention Module](#4-channel-attention-module)
  - [Dual Attention Head](#5-dual-attention-head)
  - [Sobelè¾¹ç•Œæ£€æµ‹ç®—å­](#6-sobelè¾¹ç•Œæ£€æµ‹ç®—å­)
  - [Boundary Enhancement Module](#7-boundary-enhancement-module)
  - [Boundary Injection Module](#8-boundary-injection-module)
- [ä½¿ç”¨ç¤ºä¾‹](#ä½¿ç”¨ç¤ºä¾‹)
- [æµ‹è¯•](#æµ‹è¯•)
- [æ³¨æ„äº‹é¡¹](#æ³¨æ„äº‹é¡¹)
- [å‚è€ƒæ–‡çŒ®](#å‚è€ƒæ–‡çŒ®)

---

## æ¦‚è¿°

æœ¬æ–‡ä»¶åŒ…å«ä»CTO (Convolution, Transformer, and Operator) æ¶æ„ä¸­æå–çš„å³æ’å³ç”¨æ¨¡å—ã€‚è¿™äº›æ¨¡å—å¯ä»¥ç‹¬ç«‹ä½¿ç”¨ï¼Œä¹Ÿå¯ä»¥ç»„åˆä½¿ç”¨ï¼Œé€‚ç”¨äºå„ç§æ·±åº¦å­¦ä¹ ä»»åŠ¡ï¼Œç‰¹åˆ«æ˜¯åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ã€‚

### ä¸»è¦ç‰¹ç‚¹

- âœ… **å³æ’å³ç”¨**ï¼šæ‰€æœ‰æ¨¡å—éƒ½å¯ä»¥ç‹¬ç«‹ä½¿ç”¨ï¼Œæ— éœ€ä¾èµ–å®Œæ•´çš„CTOæ¶æ„
- âœ… **æ¨¡å—åŒ–è®¾è®¡**ï¼šæ¯ä¸ªæ¨¡å—åŠŸèƒ½å•ä¸€ï¼Œæ¥å£æ¸…æ™°
- âœ… **è¯¦ç»†æ–‡æ¡£**ï¼šæ¯ä¸ªæ¨¡å—éƒ½æœ‰è¯¦ç»†çš„è¯´æ˜å’Œä½¿ç”¨ç¤ºä¾‹
- âœ… **å®Œæ•´æµ‹è¯•**ï¼šæ‰€æœ‰æ¨¡å—éƒ½åŒ…å«æµ‹è¯•å‡½æ•°ï¼Œç¡®ä¿æ­£ç¡®æ€§

---

## æ¨¡å—åˆ—è¡¨

| æ¨¡å—åç§° | åŠŸèƒ½æè¿° | å¯¹åº”è®ºæ–‡å›¾ |
|---------|---------|-----------|
| Res2Net Bottle2neck | å¤šå°ºåº¦ç‰¹å¾æå– | Fig. 3 |
| Stitch Attention | å¤šå°ºåº¦æ³¨æ„åŠ›æœºåˆ¶ | Fig. 4 |
| Position Attention Module | ä½ç½®æ³¨æ„åŠ›æ¨¡å— | - |
| Channel Attention Module | é€šé“æ³¨æ„åŠ›æ¨¡å— | - |
| Dual Attention Head | åŒæ³¨æ„åŠ›å¤´ | - |
| Sobelè¾¹ç•Œæ£€æµ‹ç®—å­ | è¾¹ç•Œæå– | - |
| Boundary Enhancement Module | è¾¹ç•Œå¢å¼ºæ¨¡å— | Fig. 2(c) |
| Boundary Injection Module | è¾¹ç•Œæ³¨å…¥æ¨¡å— | Fig. 2(c) |

---

## å¿«é€Ÿå¼€å§‹

### å®‰è£…ä¾èµ–

```bash
pip install torch torchvision numpy
```

### åŸºæœ¬ä½¿ç”¨

```python
import torch
from plug_and_play_modules import (
    Res2NetBottle2neck,
    StitchAttention,
    PositionAttentionModule,
    ChannelAttentionModule,
    DualAttentionHead,
    BoundaryEnhancementModule,
    BoundaryInjectionModule,
    get_sobel,
    run_sobel
)

# åˆ›å»ºæ¨¡å—
module = Res2NetBottle2neck(inplanes=256, planes=64, baseWidth=26, scale=4)

# å‰å‘ä¼ æ’­
x = torch.randn(2, 256, 64, 64)
out = module(x)
print(f"è¾“å‡ºå½¢çŠ¶: {out.shape}")
```

### è¿è¡Œæµ‹è¯•

```bash
python plug_and_play_modules.py
```

---

## æ¨¡å—è¯¦ç»†è¯´æ˜

### 1. Res2Net Bottle2neck

**åŠŸèƒ½**ï¼šå¤šå°ºåº¦ç‰¹å¾æå–æ¨¡å—ï¼Œé€šè¿‡å±‚æ¬¡åŒ–çš„æ®‹å·®è¿æ¥å®ç°å¤šå°ºåº¦ç‰¹å¾è¡¨ç¤ºã€‚

**å¯¹åº”ç»“æ„å›¾**ï¼šFig. 3 - Basic module of Res2Net

**ç‰¹ç‚¹**ï¼š
- å¤šå°ºåº¦ç‰¹å¾æå–ï¼ˆscaleå‚æ•°æ§åˆ¶å°ºåº¦æ•°é‡ï¼‰
- å±‚æ¬¡åŒ–çš„æ®‹å·®è¿æ¥
- å¯é…ç½®çš„åŸºç¡€å®½åº¦å’Œå°ºåº¦

**ä½¿ç”¨æ–¹æ³•**ï¼š

```python
from plug_and_play_modules import Res2NetBottle2neck

# åˆ›å»ºæ¨¡å—
module = Res2NetBottle2neck(
    inplanes=256,      # è¾“å…¥é€šé“æ•°
    planes=64,          # è¾“å‡ºé€šé“æ•°
    baseWidth=26,       # åŸºç¡€å®½åº¦
    scale=4,           # å°ºåº¦æ•°é‡ï¼ˆå¯¹åº”X1, X2, X3, X4ï¼‰
    stride=1,          # å·ç§¯æ­¥é•¿
    stype='normal'     # 'normal' æˆ– 'stage'
)

# å‰å‘ä¼ æ’­
x = torch.randn(2, 256, 64, 64)
out = module(x)  # [2, 256, 64, 64]
```

**å‚æ•°è¯´æ˜**ï¼š
- `inplanes`: è¾“å…¥é€šé“æ•°
- `planes`: è¾“å‡ºé€šé“æ•°ï¼ˆå®é™…è¾“å‡ºä¸º `planes * expansion`ï¼Œexpansion=4ï¼‰
- `baseWidth`: åŸºç¡€å®½åº¦ï¼Œæ§åˆ¶å†…éƒ¨é€šé“æ•°
- `scale`: å°ºåº¦æ•°é‡ï¼Œé€šå¸¸ä¸º4ï¼ˆå¯¹åº”X1, X2, X3, X4å››ä¸ªåˆ†æ”¯ï¼‰
- `stride`: å·ç§¯æ­¥é•¿
- `stype`: 'normal' æˆ– 'stage'ï¼Œç”¨äºæ§åˆ¶ç¬¬ä¸€ä¸ªblockçš„è¡Œä¸º

---

### 2. Stitch Attention

**åŠŸèƒ½**ï¼šå¤šå°ºåº¦æ³¨æ„åŠ›æœºåˆ¶ï¼Œé€šè¿‡ä¸åŒçš„stitch rateï¼ˆé‡‡æ ·æ­¥é•¿ï¼‰å®ç°å¤šå°ºåº¦ç‰¹å¾é‡‡æ ·å’Œæ³¨æ„åŠ›è®¡ç®—ã€‚

**å¯¹åº”ç»“æ„å›¾**ï¼šFig. 4 - Stitch-ViT

**ç‰¹ç‚¹**ï¼š
- å¤šå°ºåº¦é‡‡æ ·ï¼ˆstitch rateï¼‰
- å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶
- å¯é…ç½®çš„strideå‚æ•°

**ä½¿ç”¨æ–¹æ³•**ï¼š

```python
from plug_and_play_modules import StitchAttention

# åˆ›å»ºæ¨¡å—
module = StitchAttention(
    stride=[(2, 2), (4, 4), (8, 8)],  # é‡‡æ ·æ­¥é•¿åˆ—è¡¨
    d_model=256                        # ç‰¹å¾ç»´åº¦
)

# å‰å‘ä¼ æ’­
x = torch.randn(2, 256, 64, 64)  # æ³¨æ„ï¼šå°ºå¯¸éœ€è¦èƒ½è¢«æ‰€æœ‰strideæ•´é™¤
out = module(x)  # [2, 256, 64, 64]
```

**å‚æ•°è¯´æ˜**ï¼š
- `stride`: é‡‡æ ·æ­¥é•¿åˆ—è¡¨ï¼Œä¾‹å¦‚ `[(2,2), (4,4), (8,8)]`
  - æ¯ä¸ªå…ƒç»„ `(ws, hs)` è¡¨ç¤ºå®½åº¦å’Œé«˜åº¦æ–¹å‘çš„é‡‡æ ·æ­¥é•¿
  - **é‡è¦**ï¼šè¾“å…¥ç‰¹å¾å›¾çš„å°ºå¯¸å¿…é¡»èƒ½è¢«æ‰€æœ‰strideæ•´é™¤
- `d_model`: è¾“å…¥ç‰¹å¾ç»´åº¦

**æ³¨æ„äº‹é¡¹**ï¼š
- è¾“å…¥ç‰¹å¾å›¾çš„Hå’ŒWå¿…é¡»èƒ½è¢«æ‰€æœ‰strideä¸­çš„wså’Œhsæ•´é™¤
- ä¾‹å¦‚ï¼šå¦‚æœstride=[(2,2), (4,4), (8,8)]ï¼Œåˆ™è¾“å…¥å°ºå¯¸å¿…é¡»æ˜¯8çš„å€æ•°

---

### 3. Position Attention Module

**åŠŸèƒ½**ï¼šä½ç½®æ³¨æ„åŠ›æ¨¡å—ï¼Œæ•è·ç©ºé—´ä½ç½®é—´çš„ä¾èµ–å…³ç³»ã€‚

**ç‰¹ç‚¹**ï¼š
- ç©ºé—´ä½ç½®é—´çš„æ³¨æ„åŠ›è®¡ç®—
- è‡ªé€‚åº”æƒé‡å­¦ä¹ 

**ä½¿ç”¨æ–¹æ³•**ï¼š

```python
from plug_and_play_modules import PositionAttentionModule

# åˆ›å»ºæ¨¡å—
module = PositionAttentionModule(in_channels=256)

# å‰å‘ä¼ æ’­
x = torch.randn(2, 256, 64, 64)
out = module(x)  # [2, 256, 64, 64]
```

**å‚æ•°è¯´æ˜**ï¼š
- `in_channels`: è¾“å…¥é€šé“æ•°

---

### 4. Channel Attention Module

**åŠŸèƒ½**ï¼šé€šé“æ³¨æ„åŠ›æ¨¡å—ï¼Œæ•è·é€šé“é—´çš„ä¾èµ–å…³ç³»ã€‚

**ç‰¹ç‚¹**ï¼š
- é€šé“é—´çš„æ³¨æ„åŠ›è®¡ç®—
- è‡ªé€‚åº”æƒé‡å­¦ä¹ 

**ä½¿ç”¨æ–¹æ³•**ï¼š

```python
from plug_and_play_modules import ChannelAttentionModule

# åˆ›å»ºæ¨¡å—
module = ChannelAttentionModule()

# å‰å‘ä¼ æ’­
x = torch.randn(2, 256, 64, 64)
out = module(x)  # [2, 256, 64, 64]
```

**æ³¨æ„**ï¼šè¯¥æ¨¡å—ä¸éœ€è¦æŒ‡å®šè¾“å…¥é€šé“æ•°ï¼Œä¼šè‡ªåŠ¨ä»è¾“å…¥ç‰¹å¾ä¸­è·å–ã€‚

---

### 5. Dual Attention Head

**åŠŸèƒ½**ï¼šåŒæ³¨æ„åŠ›å¤´æ¨¡å—ï¼Œç»“åˆä½ç½®æ³¨æ„åŠ›å’Œé€šé“æ³¨æ„åŠ›ã€‚

**ç‰¹ç‚¹**ï¼š
- åŒæ—¶ä½¿ç”¨ä½ç½®æ³¨æ„åŠ›å’Œé€šé“æ³¨æ„åŠ›
- å¯é€‰çš„è¾…åŠ©è¾“å‡º

**ä½¿ç”¨æ–¹æ³•**ï¼š

```python
from plug_and_play_modules import DualAttentionHead

# åˆ›å»ºæ¨¡å—
module = DualAttentionHead(
    in_channels=256,  # è¾“å…¥é€šé“æ•°
    nclass=1,          # è¾“å‡ºç±»åˆ«æ•°
    aux=False         # æ˜¯å¦è¾“å‡ºè¾…åŠ©ç»“æœ
)

# å‰å‘ä¼ æ’­
x = torch.randn(2, 256, 64, 64)
outputs = module(x)  # å¦‚æœaux=Falseï¼Œè¿”å›tupleåŒ…å«1ä¸ªå…ƒç´ 
                     # å¦‚æœaux=Trueï¼Œè¿”å›tupleåŒ…å«3ä¸ªå…ƒç´ 
```

**å‚æ•°è¯´æ˜**ï¼š
- `in_channels`: è¾“å…¥é€šé“æ•°
- `nclass`: è¾“å‡ºç±»åˆ«æ•°
- `aux`: æ˜¯å¦è¾“å‡ºè¾…åŠ©ç»“æœï¼ˆä½ç½®æ³¨æ„åŠ›å’Œé€šé“æ³¨æ„åŠ›çš„å•ç‹¬è¾“å‡ºï¼‰

**è¿”å›å€¼**ï¼š
- å¦‚æœ `aux=False`ï¼šè¿”å› `(fusion_out,)`
- å¦‚æœ `aux=True`ï¼šè¿”å› `(fusion_out, p_out, c_out)`

---

### 6. Sobelè¾¹ç•Œæ£€æµ‹ç®—å­

**åŠŸèƒ½**ï¼šä½¿ç”¨Sobelç®—å­æå–å›¾åƒè¾¹ç•Œä¿¡æ¯ã€‚

**ç‰¹ç‚¹**ï¼š
- å¯å­¦ä¹ çš„è¾¹ç•Œæ£€æµ‹
- æ”¯æŒå¤šé€šé“è¾“å…¥

**ä½¿ç”¨æ–¹æ³•**ï¼š

```python
from plug_and_play_modules import get_sobel, run_sobel

# åˆ›å»ºSobelç®—å­
sobel_x, sobel_y = get_sobel(in_chan=3, out_chan=1)

# è¿è¡ŒSobelç®—å­
x = torch.randn(2, 3, 256, 256)
out = run_sobel(sobel_x, sobel_y, x)  # [2, 3, 256, 256]
```

**å‚æ•°è¯´æ˜**ï¼š
- `in_chan`: è¾“å…¥é€šé“æ•°
- `out_chan`: è¾“å‡ºé€šé“æ•°ï¼ˆé€šå¸¸ä¸º1ï¼‰

**æ³¨æ„**ï¼šSobelç®—å­çš„æƒé‡æ˜¯å›ºå®šçš„ï¼Œä¸å‚ä¸è®­ç»ƒã€‚

---

### 7. Boundary Enhancement Module

**åŠŸèƒ½**ï¼šè¾¹ç•Œå¢å¼ºæ¨¡å—ï¼Œèåˆå¤šå°ºåº¦è¾¹ç•Œä¿¡æ¯ã€‚

**å¯¹åº”ç»“æ„å›¾**ï¼šFig. 2(c) - Boundary Enhancement Module (BEM)

**ç‰¹ç‚¹**ï¼š
- èåˆæ·±å±‚å’Œæµ…å±‚ç‰¹å¾
- è¾“å‡ºè¾¹ç•Œç‰¹å¾å›¾

**ä½¿ç”¨æ–¹æ³•**ï¼š

```python
from plug_and_play_modules import BoundaryEnhancementModule

# åˆ›å»ºæ¨¡å—
module = BoundaryEnhancementModule()

# å‰å‘ä¼ æ’­
x1 = torch.randn(2, 256, 64, 64)   # æµ…å±‚ç‰¹å¾
x4 = torch.randn(2, 2048, 8, 8)    # æ·±å±‚ç‰¹å¾
out = module(x4, x1)  # [2, 1, 64, 64]
```

**å‚æ•°è¯´æ˜**ï¼š
- `x4`: æ·±å±‚ç‰¹å¾ [B, 2048, H/8, W/8]
- `x1`: æµ…å±‚ç‰¹å¾ [B, 256, H/4, W/4]

**è¿”å›å€¼**ï¼šè¾¹ç•Œç‰¹å¾å›¾ [B, 1, H/4, W/4]

---

### 8. Boundary Injection Module

**åŠŸèƒ½**ï¼šè¾¹ç•Œæ³¨å…¥æ¨¡å—ï¼Œå°†è¾¹ç•Œä¿¡æ¯æ³¨å…¥åˆ°è§£ç å™¨ç‰¹å¾ä¸­ã€‚

**å¯¹åº”ç»“æ„å›¾**ï¼šFig. 2(c) - Boundary Injection Module (BIM)

**ç‰¹ç‚¹**ï¼š
- å°†è¾¹ç•Œä¿¡æ¯æ³¨å…¥è§£ç å™¨
- å¤šè·¯å¾„ç‰¹å¾èåˆ

**ä½¿ç”¨æ–¹æ³•**ï¼š

```python
from plug_and_play_modules import BoundaryInjectionModule

# åˆ›å»ºæ¨¡å—
module = BoundaryInjectionModule()

# å‰å‘ä¼ æ’­
xr = torch.randn(2, 64, 64, 64)           # è§£ç å™¨ç‰¹å¾
dualattention = torch.randn(2, 64, 32, 32)  # åŒæ³¨æ„åŠ›ç‰¹å¾
out = module(xr, dualattention)  # [2, 1, 64, 64]
```

**å‚æ•°è¯´æ˜**ï¼š
- `xr`: è§£ç å™¨ç‰¹å¾ [B, 64, H, W]
- `dualattention`: åŒæ³¨æ„åŠ›ç‰¹å¾ [B, 64, H', W']

**è¿”å›å€¼**ï¼šæ³¨å…¥è¾¹ç•Œä¿¡æ¯åçš„ç‰¹å¾ [B, 1, H, W]

---

## ä½¿ç”¨ç¤ºä¾‹

### ç¤ºä¾‹1ï¼šæ„å»ºä¸€ä¸ªç®€å•çš„è¾¹ç•Œå¢å¼ºç½‘ç»œ

```python
import torch
import torch.nn as nn
from plug_and_play_modules import (
    BoundaryEnhancementModule,
    BoundaryInjectionModule,
    get_sobel,
    run_sobel
)

class SimpleBoundaryNetwork(nn.Module):
    def __init__(self):
        super().__init__()
        self.bem = BoundaryEnhancementModule()
        self.bim = BoundaryInjectionModule()
        self.sobel_x, self.sobel_y = get_sobel(256, 1)
        
    def forward(self, x1, x4, decoder_feat):
        # æå–è¾¹ç•Œ
        s1 = run_sobel(self.sobel_x, self.sobel_y, x1)
        s4 = run_sobel(self.sobel_x, self.sobel_y, x4)
        
        # è¾¹ç•Œå¢å¼º
        boundary = self.bem(s4, s1)
        
        # è¾¹ç•Œæ³¨å…¥
        output = self.bim(decoder_feat, boundary)
        
        return output

# ä½¿ç”¨
model = SimpleBoundaryNetwork()
x1 = torch.randn(2, 256, 64, 64)
x4 = torch.randn(2, 2048, 8, 8)
decoder_feat = torch.randn(2, 64, 64, 64)
out = model(x1, x4, decoder_feat)
```

### ç¤ºä¾‹2ï¼šä½¿ç”¨æ³¨æ„åŠ›æ¨¡å—å¢å¼ºç‰¹å¾

```python
import torch
import torch.nn as nn
from plug_and_play_modules import (
    PositionAttentionModule,
    ChannelAttentionModule,
    DualAttentionHead
)

class AttentionEnhancedNetwork(nn.Module):
    def __init__(self, in_channels=256):
        super().__init__()
        self.pam = PositionAttentionModule(in_channels)
        self.cam = ChannelAttentionModule()
        self.dual_head = DualAttentionHead(in_channels, nclass=1, aux=False)
        
    def forward(self, x):
        # ä½ç½®æ³¨æ„åŠ›
        pam_out = self.pam(x)
        
        # é€šé“æ³¨æ„åŠ›
        cam_out = self.cam(x)
        
        # åŒæ³¨æ„åŠ›èåˆ
        dual_out = self.dual_head(x)
        
        # ç‰¹å¾èåˆ
        enhanced = pam_out + cam_out + dual_out[0]
        
        return enhanced

# ä½¿ç”¨
model = AttentionEnhancedNetwork()
x = torch.randn(2, 256, 64, 64)
out = model(x)
```

### ç¤ºä¾‹3ï¼šå¤šå°ºåº¦ç‰¹å¾æå–

```python
import torch
import torch.nn as nn
from plug_and_play_modules import (
    Res2NetBottle2neck,
    StitchAttention
)

class MultiScaleNetwork(nn.Module):
    def __init__(self):
        super().__init__()
        self.res2net = Res2NetBottle2neck(
            inplanes=256, planes=64, baseWidth=26, scale=4
        )
        self.stitch_attn = StitchAttention(
            stride=[(2, 2), (4, 4), (8, 8)],
            d_model=256
        )
        
    def forward(self, x):
        # Res2Netå¤šå°ºåº¦ç‰¹å¾
        res2net_out = self.res2net(x)
        
        # Stitch Attentionå¤šå°ºåº¦æ³¨æ„åŠ›
        stitch_out = self.stitch_attn(x)
        
        # ç‰¹å¾èåˆ
        fused = res2net_out + stitch_out
        
        return fused

# ä½¿ç”¨
model = MultiScaleNetwork()
x = torch.randn(2, 256, 64, 64)  # æ³¨æ„ï¼š64èƒ½è¢«2, 4, 8æ•´é™¤
out = model(x)
```

---

## æµ‹è¯•

è¿è¡Œå®Œæ•´æµ‹è¯•ï¼š

```bash
python plug_and_play_modules.py
```

æµ‹è¯•è¾“å‡ºç¤ºä¾‹ï¼š

```
============================================================
CTOæ¶æ„å³æ’å³ç”¨æ¨¡å—æµ‹è¯•
============================================================

==================================================
æµ‹è¯• Res2Net Bottle2neck æ¨¡å—
==================================================
è¾“å…¥å½¢çŠ¶: torch.Size([2, 256, 64, 64])
è¾“å‡ºå½¢çŠ¶: torch.Size([2, 256, 64, 64])
âœ“ Res2Net Bottle2neck æµ‹è¯•é€šè¿‡

... (å…¶ä»–æ¨¡å—æµ‹è¯•)

============================================================
âœ“ æ‰€æœ‰æ¨¡å—æµ‹è¯•é€šè¿‡ï¼
============================================================
```

---

## æ³¨æ„äº‹é¡¹

### 1. Stitch Attentionçš„å°ºå¯¸è¦æ±‚

**é‡è¦**ï¼šä½¿ç”¨ `StitchAttention` æ—¶ï¼Œè¾“å…¥ç‰¹å¾å›¾çš„å°ºå¯¸å¿…é¡»èƒ½è¢«æ‰€æœ‰strideæ•´é™¤ã€‚

```python
# âœ… æ­£ç¡®ï¼š64èƒ½è¢«2, 4, 8æ•´é™¤
x = torch.randn(2, 256, 64, 64)
module = StitchAttention(stride=[(2,2), (4,4), (8,8)], d_model=256)

# âŒ é”™è¯¯ï¼š64ä¸èƒ½è¢«3æ•´é™¤
x = torch.randn(2, 256, 64, 64)
module = StitchAttention(stride=[(2,2), (3,3), (4,4)], d_model=256)
```

### 2. é€šé“æ•°åŒ¹é…

ä½¿ç”¨æ¨¡å—ç»„åˆæ—¶ï¼Œæ³¨æ„é€šé“æ•°çš„åŒ¹é…ï¼š

```python
# ç¡®ä¿é€šé“æ•°åŒ¹é…
x1 = torch.randn(2, 256, 64, 64)   # 256é€šé“
x4 = torch.randn(2, 2048, 8, 8)    # 2048é€šé“
bem = BoundaryEnhancementModule()  # å†…éƒ¨å¤„ç†é€šé“æ•°è½¬æ¢
out = bem(x4, x1)
```

### 3. è®¾å¤‡é€‰æ‹©

æ‰€æœ‰æ¨¡å—éƒ½æ”¯æŒCPUå’ŒGPUï¼š

```python
# CPU
module = Res2NetBottle2neck(inplanes=256, planes=64)
x = torch.randn(2, 256, 64, 64)
out = module(x)

# GPU
device = torch.device('cuda')
module = module.to(device)
x = x.to(device)
out = module(x)
```

### 4. è®­ç»ƒæ¨¡å¼

æ¨¡å—é»˜è®¤å¤„äºè®­ç»ƒæ¨¡å¼ï¼Œæ¨ç†æ—¶éœ€è¦è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ï¼š

```python
module.eval()
with torch.no_grad():
    out = module(x)
```

---

## å‚è€ƒæ–‡çŒ®

1. **CTOè®ºæ–‡**ï¼š
   - Lin, Y., Zhang, D., Fang, X., Chen, Y., Cheng, K. T., & Chen, H. (2025). Rethinking boundary detection in deep learning-based medical image segmentation. *Medical Image Analysis*.

2. **Res2Netè®ºæ–‡**ï¼š
   - Gao, S. H., Cheng, M. M., Zhao, K., Zhang, X. Y., Yang, M. H., & Torr, P. H. (2019). Res2Net: A new multi-scale backbone architecture. *IEEE transactions on pattern analysis and machine intelligence*.

3. **ç›¸å…³ä»£ç **ï¼š
   - CTOå®˜æ–¹ä»£ç ï¼šhttps://github.com/xiaofang007/CTO

---

## è®¸å¯è¯

æœ¬ä»£ç éµå¾ªåŸCTOé¡¹ç›®çš„è®¸å¯è¯ã€‚

---

## æ›´æ–°æ—¥å¿—

- **2025-01-XX**: åˆå§‹ç‰ˆæœ¬ï¼Œæå–8ä¸ªå³æ’å³ç”¨æ¨¡å—
  - Res2Net Bottle2neck
  - Stitch Attention
  - Position/Channel Attention Modules
  - Dual Attention Head
  - Sobelè¾¹ç•Œæ£€æµ‹ç®—å­
  - Boundary Enhancement/Injection Modules

---

## è´¡çŒ®

æ¬¢è¿æäº¤Issueå’ŒPull Requestï¼

---

## è”ç³»æ–¹å¼

å¦‚æœ‰é—®é¢˜ï¼Œè¯·å‚è€ƒåŸCTOé¡¹ç›®çš„Issueé¡µé¢ã€‚

