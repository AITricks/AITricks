# ABCNet å³æ’å³ç”¨æ³¨æ„åŠ›æ¨¡å—ä½¿ç”¨è¯´æ˜
# ABCNet Plug-and-Play Attention Modules User Guide

## ğŸ“‹ ç›®å½• (Table of Contents)
- [æ¦‚è¿° (Overview)](#æ¦‚è¿°-overview)
- [æ¨¡å—ä»‹ç» (Module Introduction)](#æ¨¡å—ä»‹ç»-module-introduction)
- [å¿«é€Ÿå¼€å§‹ (Quick Start)](#å¿«é€Ÿå¼€å§‹-quick-start)
- [è¯¦ç»†ä½¿ç”¨æŒ‡å— (Detailed Usage Guide)](#è¯¦ç»†ä½¿ç”¨æŒ‡å—-detailed-usage-guide)
- [API å‚è€ƒ (API Reference)](#api-å‚è€ƒ-api-reference)
- [ç¤ºä¾‹ä»£ç  (Examples)](#ç¤ºä¾‹ä»£ç -examples)
- [å¸¸è§é—®é¢˜ (FAQ)](#å¸¸è§é—®é¢˜-faq)

---

## æ¦‚è¿° (Overview)

`ABCNet_block.py` æä¾›äº† ABC ç½‘ç»œçš„æ ¸å¿ƒæ³¨æ„åŠ›æ¨¡å—ï¼Œå®ç°äº†çœŸæ­£çš„**å³æ’å³ç”¨**è®¾è®¡ï¼š

âœ… **æ— éœ€å¤–éƒ¨ä¾èµ–**ï¼šä»…ä½¿ç”¨ PyTorch æ ‡å‡†åº“  
âœ… **è‡ªåŠ¨å‚æ•°æ¨æ–­**ï¼šæ— éœ€æ‰‹åŠ¨æŒ‡å®šç‰¹å¾å›¾å°ºå¯¸  
âœ… **æ”¯æŒä»»æ„è¾“å…¥å°ºå¯¸**ï¼šè‡ªåŠ¨é€‚é…ä¸åŒåˆ†è¾¨ç‡çš„è¾“å…¥  
âœ… **çµæ´»çš„æ‰¹å¤„ç†**ï¼šæ”¯æŒä»»æ„æ‰¹æ¬¡å¤§å°  

`ABCNet_block.py` provides core attention modules of ABC network with true **plug-and-play** design:

âœ… **No external dependencies**: Only uses PyTorch standard library  
âœ… **Auto parameter inference**: No need to manually specify feature map sizes  
âœ… **Support arbitrary input sizes**: Automatically adapts to different resolutions  
âœ… **Flexible batch processing**: Supports arbitrary batch sizes  

---

## æ¨¡å—ä»‹ç» (Module Introduction)

### 1. BilinearAttention (BAM) - åŒçº¿æ€§æ³¨æ„åŠ›æ¨¡å—

ABC ç½‘ç»œçš„æ ¸å¿ƒåˆ›æ–°ï¼Œé€šè¿‡åŒçº¿æ€§ç›¸å…³æ€§è®¡ç®—ç©ºé—´æ³¨æ„åŠ›ã€‚

**ç‰¹ç‚¹ (Features)**:
- è‡ªåŠ¨é€‚é…ä»»æ„è¾“å…¥å°ºå¯¸ (H, W)
- æ— éœ€é¢„å…ˆæŒ‡å®šç‰¹å¾å›¾ç»´åº¦
- è½»é‡çº§è®¾è®¡ï¼Œè®¡ç®—æ•ˆç‡é«˜

**ä½¿ç”¨åœºæ™¯ (Use Cases)**:
- ä½œä¸ºç‹¬ç«‹çš„æ³¨æ„åŠ›æ¨¡å—æ’å…¥åˆ°ç°æœ‰ç½‘ç»œä¸­
- ä¸å…¶ä»–æ³¨æ„åŠ›æœºåˆ¶ç»„åˆä½¿ç”¨
- ç”¨äºç‰¹å¾å¢å¼ºå’Œç©ºé—´å…³ç³»å»ºæ¨¡

### 2. ConvAttention - å·ç§¯æ³¨æ„åŠ›æ¨¡å—

ç»“åˆæ™®é€šå·ç§¯å’Œæ‰©å¼ å·ç§¯çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œç”¨äºæå–å¤šå°ºåº¦ç‰¹å¾ã€‚

**ç‰¹ç‚¹ (Features)**:
- èåˆå±€éƒ¨å’Œå…¨å±€ä¿¡æ¯
- é€šè¿‡æ‰©å¼ å·ç§¯æ•è·å¤šå°ºåº¦ä¸Šä¸‹æ–‡
- åŒ…å«æ®‹å·®è¿æ¥ï¼Œè®­ç»ƒç¨³å®š

**ä½¿ç”¨åœºæ™¯ (Use Cases)**:
- ç¼–ç å™¨ä¸­çš„ç‰¹å¾æå–
- å¤šå°ºåº¦ç‰¹å¾èåˆ
- å¢å¼ºç½‘ç»œçš„ç‰¹å¾è¡¨ç¤ºèƒ½åŠ›

### 3. ConvTransformerBlock (CLFT) - å·ç§¯çº¿æ€§èåˆTransformer

ABC ç½‘ç»œç¼–ç å™¨ä¸­çš„æ ¸å¿ƒæ¨¡å—ï¼Œç»“åˆäº†å·ç§¯æ³¨æ„åŠ›å’Œå‰é¦ˆç½‘ç»œã€‚

**ç‰¹ç‚¹ (Features)**:
- å®Œæ•´çš„ Transformer ç»“æ„
- æ”¯æŒé€šé“æ•°å˜åŒ–ï¼ˆè¾“å…¥è¾“å‡ºé€šé“å¯ä»¥ä¸åŒï¼‰
- é€‚åˆç”¨äºç¼–ç å™¨é˜¶æ®µ

**ä½¿ç”¨åœºæ™¯ (Use Cases)**:
- UNet é£æ ¼çš„ç¼–ç å™¨
- ç‰¹å¾æå–å’Œå˜æ¢
- å¤šå°ºåº¦ç‰¹å¾å¤„ç†

### 4. SimplifiedBAM - ç®€åŒ–ç‰ˆåŒçº¿æ€§æ³¨æ„åŠ›

è½»é‡çº§åŒçº¿æ€§æ³¨æ„åŠ›æ¨¡å—ï¼Œé€‚åˆèµ„æºå—é™çš„åœºæ™¯ã€‚

**ç‰¹ç‚¹ (Features)**:
- æ›´ä½çš„è®¡ç®—å¤æ‚åº¦
- ä¿æŒæ ¸å¿ƒçš„åŒçº¿æ€§æ³¨æ„åŠ›æœºåˆ¶
- é€‚åˆç§»åŠ¨ç«¯æˆ–è¾¹ç¼˜è®¾å¤‡

**ä½¿ç”¨åœºæ™¯ (Use Cases)**:
- èµ„æºå—é™çš„åœºæ™¯
- å®æ—¶æ¨ç†åº”ç”¨
- è½»é‡çº§æ¨¡å‹è®¾è®¡

### 5. UCDC - Uå½¢å·ç§¯-æ‰©å¼ å·ç§¯æ¨¡å—

Uå½¢ç»“æ„çš„å·ç§¯-æ‰©å¼ å·ç§¯æ¨¡å—ï¼Œç”¨äºç“¶é¢ˆå±‚å’Œè§£ç å™¨é˜¶æ®µã€‚

**ç‰¹ç‚¹ (Features)**:
- Uå½¢ç»“æ„ï¼ŒåŒ…å«å†…éƒ¨å’Œå¤–éƒ¨skip connections
- å¤šå°ºåº¦æ‰©å¼ å·ç§¯ï¼ˆdilation rates: 2, 4, 2ï¼‰
- æ•è·å¤šå°ºåº¦ä¸Šä¸‹æ–‡ä¿¡æ¯
- æ”¯æŒé€šé“æ•°å˜åŒ–

**ä½¿ç”¨åœºæ™¯ (Use Cases)**:
- ç“¶é¢ˆå±‚ï¼ˆbottleneck layerï¼‰
- è§£ç å™¨é˜¶æ®µ
- éœ€è¦å¤šå°ºåº¦ç‰¹å¾èåˆçš„åœºæ™¯
- ABCç½‘ç»œçš„å®Œæ•´å®ç°

---

## å¿«é€Ÿå¼€å§‹ (Quick Start)

### åŸºç¡€ä½¿ç”¨ (Basic Usage)

```python
import torch
from ABCNet_block import BilinearAttention, ConvAttention, ConvTransformerBlock, UCDC

# åˆ›å»ºè¾“å…¥ç‰¹å¾å›¾
x = torch.randn(2, 64, 32, 32)  # (batch, channels, height, width)

# 1. ä½¿ç”¨ BilinearAttention
bam = BilinearAttention(in_dim=64)
att_out = bam(x)  # è¾“å‡ºå½¢çŠ¶: (2, 64, 32, 32)

# 2. ä½¿ç”¨ ConvAttention
conv_att = ConvAttention(in_dim=64)
conv_out = conv_att(x)  # è¾“å‡ºå½¢çŠ¶: (2, 64, 32, 32)

# 3. ä½¿ç”¨ ConvTransformerBlock
clft = ConvTransformerBlock(in_dim=64, out_dim=128)
clft_out = clft(x)  # è¾“å‡ºå½¢çŠ¶: (2, 128, 32, 32)

# 4. ä½¿ç”¨ UCDCæ¨¡å—
ucdc = UCDC(in_ch=64, out_ch=128)
ucdc_out = ucdc(x)  # è¾“å‡ºå½¢çŠ¶: (2, 128, 32, 32)
```

### é›†æˆåˆ°ç°æœ‰ç½‘ç»œ (Integration into Existing Networks)

```python
import torch.nn as nn
from ABCNet_block import ConvTransformerBlock, UCDC

class MyNetwork(nn.Module):
    def __init__(self):
        super().__init__()
        # ç¼–ç å™¨
        self.encoder1 = nn.Conv2d(3, 64, 3, padding=1)
        self.encoder2 = ConvTransformerBlock(64, 128)
        self.encoder3 = ConvTransformerBlock(128, 256)
        
        # ç“¶é¢ˆå±‚ï¼šä½¿ç”¨UCDCæ¨¡å—
        self.bottleneck = UCDC(256, 512)
        
        # è§£ç å™¨
        self.decoder = nn.Conv2d(512, 1, 1)
        
    def forward(self, x):
        x = self.encoder1(x)
        x = self.encoder2(x)
        x = self.encoder3(x)
        x = self.bottleneck(x)  # UCDCæ¨¡å—
        x = self.decoder(x)
        return x
```

---

## è¯¦ç»†ä½¿ç”¨æŒ‡å— (Detailed Usage Guide)

### 1. BilinearAttention ä½¿ç”¨

#### åŸºæœ¬ç”¨æ³•

```python
from ABCNet_block import BilinearAttention

# åˆ›å»ºæ¨¡å—ï¼ˆåªéœ€æŒ‡å®šé€šé“æ•°ï¼‰
bam = BilinearAttention(in_dim=64, reduction_ratio=4)

# å‰å‘ä¼ æ’­ï¼ˆæ”¯æŒä»»æ„å°ºå¯¸ï¼‰
x1 = torch.randn(1, 64, 32, 32)   # å°å°ºå¯¸
x2 = torch.randn(1, 64, 256, 256) # å¤§å°ºå¯¸
x3 = torch.randn(1, 64, 64, 128)  # éæ­£æ–¹å½¢

out1 = bam(x1)  # (1, 64, 32, 32)
out2 = bam(x2)  # (1, 64, 256, 256)
out3 = bam(x3)  # (1, 64, 64, 128)
```

#### å‚æ•°è¯´æ˜

- `in_dim` (int): è¾“å…¥é€šé“æ•°ï¼Œå¿…éœ€å‚æ•°
- `reduction_ratio` (int): æ³¨æ„åŠ›é™ç»´æ¯”ä¾‹ï¼Œé»˜è®¤ 4

#### æ³¨æ„äº‹é¡¹

- è¾“å…¥å¿…é¡»æ˜¯ 4D å¼ é‡ï¼š`(B, C, H, W)`
- è¾“å‡ºå½¢çŠ¶ä¸è¾“å…¥ç›¸åŒï¼š`(B, C, H, W)`
- æ”¯æŒä»»æ„æ‰¹æ¬¡å¤§å°å’Œç©ºé—´å°ºå¯¸

### 2. ConvAttention ä½¿ç”¨

#### åŸºæœ¬ç”¨æ³•

```python
from ABCNet_block import ConvAttention

# åˆ›å»ºæ¨¡å—
conv_att = ConvAttention(in_dim=64, reduction_ratio=4)

# å‰å‘ä¼ æ’­
x = torch.randn(2, 64, 64, 64)
out = conv_att(x)  # (2, 64, 64, 64)
```

#### å†…éƒ¨ç»“æ„

```
è¾“å…¥ (x)
  â”œâ”€ Conv åˆ†æ”¯ â†’ q
  â”œâ”€ DConv åˆ†æ”¯ â†’ k
  â””â”€ BilinearAttention â†’ att
  
v = q + k
out = Î³ * (att * v) + v + x  (æ®‹å·®è¿æ¥)
```

#### å‚æ•°è¯´æ˜

- `in_dim` (int): è¾“å…¥é€šé“æ•°ï¼Œå¿…éœ€å‚æ•°
- `reduction_ratio` (int): BAM çš„é™ç»´æ¯”ä¾‹ï¼Œé»˜è®¤ 4

### 3. ConvTransformerBlock (CLFT) ä½¿ç”¨

#### åŸºæœ¬ç”¨æ³•

```python
from ABCNet_block import ConvTransformerBlock

# åˆ›å»ºæ¨¡å—ï¼ˆæ”¯æŒé€šé“æ•°å˜åŒ–ï¼‰
clft = ConvTransformerBlock(in_dim=64, out_dim=128, reduction_ratio=4)

# å‰å‘ä¼ æ’­
x = torch.randn(2, 64, 32, 32)
out = clft(x)  # (2, 128, 32, 32)
```

#### å†…éƒ¨ç»“æ„

```
è¾“å…¥ (x)
  â””â”€ ConvAttention â†’ x'
     â””â”€ FeedForward â†’ out (é€šé“æ•°å˜åŒ–)
```

#### å‚æ•°è¯´æ˜

- `in_dim` (int): è¾“å…¥é€šé“æ•°ï¼Œå¿…éœ€å‚æ•°
- `out_dim` (int): è¾“å‡ºé€šé“æ•°ï¼Œå¿…éœ€å‚æ•°
- `reduction_ratio` (int): æ³¨æ„åŠ›é™ç»´æ¯”ä¾‹ï¼Œé»˜è®¤ 4

### 4. SimplifiedBAM ä½¿ç”¨

#### åŸºæœ¬ç”¨æ³•

```python
from ABCNet_block import SimplifiedBAM

# åˆ›å»ºæ¨¡å—
simple_bam = SimplifiedBAM(in_dim=64, reduction_ratio=8)

# å‰å‘ä¼ æ’­
x = torch.randn(2, 64, 64, 64)
out = simple_bam(x)  # (2, 64, 64, 64)
```

#### é€‚ç”¨åœºæ™¯

- èµ„æºå—é™çš„åœºæ™¯
- éœ€è¦å¿«é€Ÿæ¨ç†çš„åº”ç”¨
- è½»é‡çº§æ¨¡å‹è®¾è®¡

### 5. UCDC ä½¿ç”¨

#### åŸºæœ¬ç”¨æ³•

```python
from ABCNet_block import UCDC

# åˆ›å»ºæ¨¡å—ï¼ˆæ”¯æŒé€šé“æ•°å˜åŒ–ï¼‰
ucdc = UCDC(in_ch=64, out_ch=128)

# å‰å‘ä¼ æ’­
x = torch.randn(2, 64, 32, 32)
out = ucdc(x)  # (2, 128, 32, 32)
```

#### å†…éƒ¨ç»“æ„

```
è¾“å…¥ (x)
  â†“
Conv (åˆå§‹å·ç§¯) â†’ x1
  â†“
D.C.(r=2) â†’ dx1 â”€â”€â”
  â†“                â”‚ (å†…éƒ¨skip connection)
D.C.(r=4) â†’ dx2   â”‚
  â†“                â”‚
D.C.(r=2) â† concat(dx1, dx2) â†’ dx3
  â†“
Conv (æœ€ç»ˆå·ç§¯) â† concat(x1, dx3)
  â†“
è¾“å‡º (out)
```

#### å‚æ•°è¯´æ˜

- `in_ch` (int): è¾“å…¥é€šé“æ•°ï¼Œå¿…éœ€å‚æ•°
- `out_ch` (int): è¾“å‡ºé€šé“æ•°ï¼Œå¿…éœ€å‚æ•°

#### ç‰¹ç‚¹

- **Uå½¢ç»“æ„**: åŒ…å«å†…éƒ¨å’Œå¤–éƒ¨skip connectionsï¼Œä¿æŒä¿¡æ¯æµ
- **å¤šå°ºåº¦æ‰©å¼ å·ç§¯**: ä½¿ç”¨ä¸åŒçš„dilation rates (2, 4, 2) æ•è·å¤šå°ºåº¦ç‰¹å¾
- **é€šé“æ•°çµæ´»**: æ”¯æŒè¾“å…¥è¾“å‡ºé€šé“æ•°çš„å˜åŒ–
- **å³æ’å³ç”¨**: æ— éœ€é¢„å…ˆæŒ‡å®šè¾“å…¥å°ºå¯¸ï¼Œè‡ªåŠ¨é€‚é…

#### é€‚ç”¨åœºæ™¯

- ç“¶é¢ˆå±‚ï¼ˆbottleneck layerï¼‰
- è§£ç å™¨é˜¶æ®µ
- éœ€è¦å¤šå°ºåº¦ä¸Šä¸‹æ–‡ä¿¡æ¯çš„åœºæ™¯
- ABCç½‘ç»œçš„å®Œæ•´å®ç°

#### åœ¨ABCç½‘ç»œä¸­çš„ä½¿ç”¨

```python
# ç“¶é¢ˆå±‚
bottleneck = UCDC(in_ch=256, out_ch=512)

# è§£ç å™¨é˜¶æ®µ
decoder_stage = UCDC(in_ch=512, out_ch=256)
```

---

## API å‚è€ƒ (API Reference)

### BilinearAttention

```python
class BilinearAttention(nn.Module):
    """
    BAM (Bilinear Attention Module) - åŒçº¿æ€§æ³¨æ„åŠ›æ¨¡å—
    
    Args:
        in_dim (int): è¾“å…¥é€šé“æ•°
        reduction_ratio (int): æ³¨æ„åŠ›é™ç»´æ¯”ä¾‹ï¼Œé»˜è®¤ 4
    """
    def __init__(self, in_dim, reduction_ratio=4):
        ...
    
    def forward(self, x):
        """
        Args:
            x (torch.Tensor): è¾“å…¥ç‰¹å¾å›¾ (B, C, H, W)
        Returns:
            torch.Tensor: æ³¨æ„åŠ›åŠ æƒçš„è¾“å‡º (B, C, H, W)
        """
        ...
```

### ConvAttention

```python
class ConvAttention(nn.Module):
    """
    å·ç§¯æ³¨æ„åŠ›æ¨¡å—ï¼šç»“åˆæ™®é€šå·ç§¯å’Œæ‰©å¼ å·ç§¯çš„æ³¨æ„åŠ›æœºåˆ¶
    
    Args:
        in_dim (int): è¾“å…¥é€šé“æ•°
        reduction_ratio (int): æ³¨æ„åŠ›é™ç»´æ¯”ä¾‹ï¼Œé»˜è®¤ 4
    """
    def __init__(self, in_dim, reduction_ratio=4):
        ...
    
    def forward(self, x):
        """
        Args:
            x (torch.Tensor): è¾“å…¥ç‰¹å¾å›¾ (B, C, H, W)
        Returns:
            torch.Tensor: æ³¨æ„åŠ›åŠ æƒçš„è¾“å‡º (B, C, H, W)
        """
        ...
```

### ConvTransformerBlock

```python
class ConvTransformerBlock(nn.Module):
    """
    CLFT (Convolution Linear Fusion Transformer) - å·ç§¯çº¿æ€§èåˆTransformer
    
    Args:
        in_dim (int): è¾“å…¥é€šé“æ•°
        out_dim (int): è¾“å‡ºé€šé“æ•°
        reduction_ratio (int): æ³¨æ„åŠ›é™ç»´æ¯”ä¾‹ï¼Œé»˜è®¤ 4
    """
    def __init__(self, in_dim, out_dim, reduction_ratio=4):
        ...
    
    def forward(self, x):
        """
        Args:
            x (torch.Tensor): è¾“å…¥ç‰¹å¾å›¾ (B, in_dim, H, W)
        Returns:
            torch.Tensor: å˜æ¢åçš„ç‰¹å¾å›¾ (B, out_dim, H, W)
        """
        ...
```

### SimplifiedBAM

```python
class SimplifiedBAM(nn.Module):
    """
    ç®€åŒ–ç‰ˆBAM - è½»é‡çº§åŒçº¿æ€§æ³¨æ„åŠ›æ¨¡å—
    
    Args:
        in_dim (int): è¾“å…¥é€šé“æ•°
        reduction_ratio (int): æ³¨æ„åŠ›é™ç»´æ¯”ä¾‹ï¼Œé»˜è®¤ 8
    """
    def __init__(self, in_dim, reduction_ratio=8):
        ...
    
    def forward(self, x):
        """
        Args:
            x (torch.Tensor): è¾“å…¥ç‰¹å¾å›¾ (B, C, H, W)
        Returns:
            torch.Tensor: æ³¨æ„åŠ›åŠ æƒçš„è¾“å‡º (B, C, H, W)
        """
        ...
```

### UCDC

```python
class UCDC(nn.Module):
    """
    UCDC (U-shaped Convolution-Dilated Convolution) - Uå½¢å·ç§¯-æ‰©å¼ å·ç§¯æ¨¡å—
    
    Args:
        in_ch (int): è¾“å…¥é€šé“æ•°
        out_ch (int): è¾“å‡ºé€šé“æ•°
    """
    def __init__(self, in_ch, out_ch):
        ...
    
    def forward(self, x):
        """
        Args:
            x (torch.Tensor): è¾“å…¥ç‰¹å¾å›¾ (B, in_ch, H, W)
        Returns:
            torch.Tensor: è¾“å‡ºç‰¹å¾å›¾ (B, out_ch, H, W)
        """
        ...
```

---

## ç¤ºä¾‹ä»£ç  (Examples)

### ç¤ºä¾‹ 1ï¼šåŸºç¡€ä½¿ç”¨

```python
import torch
from ABCNet_block import BilinearAttention, ConvAttention, ConvTransformerBlock, UCDC

# åˆ›å»ºè¾“å…¥
x = torch.randn(2, 64, 32, 32)
print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")

# BAMæ¨¡å—
bam = BilinearAttention(in_dim=64)
bam_out = bam(x)
print(f"BAMè¾“å‡ºå½¢çŠ¶: {bam_out.shape}")

# CLFTæ¨¡å—
clft = ConvTransformerBlock(in_dim=64, out_dim=128)
clft_out = clft(x)
print(f"CLFTè¾“å‡ºå½¢çŠ¶: {clft_out.shape}")

# UCDCæ¨¡å—
ucdc = UCDC(in_ch=64, out_ch=128)
ucdc_out = ucdc(x)
print(f"UCDCè¾“å‡ºå½¢çŠ¶: {ucdc_out.shape}")
```

### ç¤ºä¾‹ 2ï¼šé›†æˆåˆ°UNet

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from ABCNet_block import ConvTransformerBlock, ConvAttention, UCDC

class UNetWithABC(nn.Module):
    def __init__(self, in_channels=3, out_channels=1):
        super().__init__()
        
        # ç¼–ç å™¨
        self.enc1 = nn.Conv2d(in_channels, 64, 3, padding=1)
        self.enc2 = ConvTransformerBlock(64, 128)
        self.enc3 = ConvTransformerBlock(128, 256)
        self.enc4 = ConvTransformerBlock(256, 512)
        
        # ç“¶é¢ˆå±‚ï¼šä½¿ç”¨UCDCæ¨¡å—
        self.bottleneck = UCDC(512, 1024)
        
        # è§£ç å™¨ï¼šä½¿ç”¨UCDCæ¨¡å—
        self.dec4 = UCDC(1024, 512)
        self.dec3 = ConvTransformerBlock(512, 256)
        self.dec2 = ConvTransformerBlock(256, 128)
        self.dec1 = nn.Conv2d(128, 64, 3, padding=1)
        self.final = nn.Conv2d(64, out_channels, 1)
        
        self.pool = nn.MaxPool2d(2)
        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)
        
    def forward(self, x):
        # ç¼–ç å™¨
        e1 = F.relu(self.enc1(x))
        e2 = self.pool(e1)
        e2 = self.enc2(e2)
        e3 = self.pool(e2)
        e3 = self.enc3(e3)
        e4 = self.pool(e3)
        e4 = self.enc4(e4)
        
        # ç“¶é¢ˆå±‚ï¼šUCDCæ¨¡å—
        b = self.pool(e4)
        b = self.bottleneck(b)
        
        # è§£ç å™¨
        d4 = self.up(b)
        d4 = torch.cat([e4, d4], dim=1)
        d4 = self.dec4(d4)  # UCDCæ¨¡å—
        
        d3 = self.up(d4)
        d3 = torch.cat([e3, d3], dim=1)
        d3 = self.dec3(d3)
        
        d2 = self.up(d3)
        d2 = torch.cat([e2, d2], dim=1)
        d2 = self.dec2(d2)
        
        d1 = self.up(d2)
        d1 = torch.cat([e1, d1], dim=1)
        d1 = F.relu(self.dec1(d1))
        out = self.final(d1)
        
        return out

# æµ‹è¯•
model = UNetWithABC()
x = torch.randn(1, 3, 256, 256)
out = model(x)
print(f"è¾“å‡ºå½¢çŠ¶: {out.shape}")  # (1, 1, 256, 256)
```

### ç¤ºä¾‹ 2.5ï¼šUCDCæ¨¡å—è¯¦ç»†ä½¿ç”¨

```python
import torch
import torch.nn as nn
from ABCNet_block import UCDC

# åŸºç¡€ä½¿ç”¨
ucdc = UCDC(in_ch=64, out_ch=128)
x = torch.randn(2, 64, 32, 32)
out = ucdc(x)
print(f"UCDC - è¾“å…¥: {x.shape}, è¾“å‡º: {out.shape}")

# åœ¨å®Œæ•´ç½‘ç»œä¸­ä½¿ç”¨
class NetworkWithUCDC(nn.Module):
    def __init__(self):
        super().__init__()
        self.encoder = nn.Conv2d(3, 256, 3, padding=1)
        # ç“¶é¢ˆå±‚ï¼šä½¿ç”¨UCDC
        self.bottleneck = UCDC(256, 512)
        # è§£ç å™¨ï¼šä¹Ÿä½¿ç”¨UCDC
        self.decoder = UCDC(512, 256)
        self.output = nn.Conv2d(256, 1, 1)
        
    def forward(self, x):
        x = self.encoder(x)
        x = self.bottleneck(x)  # UCDCæ¨¡å—
        x = self.decoder(x)      # UCDCæ¨¡å—
        x = self.output(x)
        return x

model = NetworkWithUCDC()
x = torch.randn(1, 3, 128, 128)
out = model(x)
print(f"å®Œæ•´ç½‘ç»œè¾“å‡º: {out.shape}")
```

### ç¤ºä¾‹ 3ï¼šå¤šå°ºåº¦ç‰¹å¾èåˆ

```python
import torch
import torch.nn as nn
from ABCNet_block import ConvAttention

class MultiScaleFusion(nn.Module):
    def __init__(self, in_dim=64):
        super().__init__()
        self.attention = ConvAttention(in_dim)
        self.conv = nn.Conv2d(in_dim, in_dim, 3, padding=1)
        
    def forward(self, x):
        # åº”ç”¨æ³¨æ„åŠ›
        att_out = self.attention(x)
        # èåˆ
        out = self.conv(att_out + x)
        return out

# æµ‹è¯•
model = MultiScaleFusion(64)
x = torch.randn(2, 64, 128, 128)
out = model(x)
print(f"è¾“å‡ºå½¢çŠ¶: {out.shape}")  # (2, 64, 128, 128)
```

### ç¤ºä¾‹ 4ï¼šæ€§èƒ½æµ‹è¯•

```python
import torch
import time
from ABCNet_block import BilinearAttention, ConvAttention, ConvTransformerBlock, UCDC

# æµ‹è¯•å‚æ•°
batch_size = 4
channels = 64
height, width = 64, 64

x = torch.randn(batch_size, channels, height, width)

# æµ‹è¯•ä¸åŒæ¨¡å—
modules = {
    'BAM': BilinearAttention(channels),
    'ConvAttention': ConvAttention(channels),
    'CLFT': ConvTransformerBlock(channels, channels),
    'UCDC': UCDC(channels, channels),
}

for name, module in modules.items():
    # é¢„çƒ­
    for _ in range(10):
        _ = module(x)
    
    # è®¡æ—¶
    start_time = time.time()
    for _ in range(100):
        _ = module(x)
    end_time = time.time()
    
    avg_time = (end_time - start_time) / 100 * 1000  # æ¯«ç§’
    print(f"{name} å¹³å‡æ¨ç†æ—¶é—´: {avg_time:.2f} ms")
    
    # è®¡ç®—å‚æ•°é‡
    total_params = sum(p.numel() for p in module.parameters())
    print(f"{name} å‚æ•°é‡: {total_params:,}")
```

---

## å¸¸è§é—®é¢˜ (FAQ)

### Q1: å¦‚ä½•é€‰æ‹©åˆé€‚çš„æ¨¡å—ï¼Ÿ

**A:** 
- **BilinearAttention**: éœ€è¦è½»é‡çº§ç©ºé—´æ³¨æ„åŠ›æ—¶ä½¿ç”¨
- **ConvAttention**: éœ€è¦å¤šå°ºåº¦ç‰¹å¾èåˆæ—¶ä½¿ç”¨
- **ConvTransformerBlock**: ç”¨äºç¼–ç å™¨é˜¶æ®µï¼Œéœ€è¦æ”¹å˜é€šé“æ•°æ—¶ä½¿ç”¨
- **SimplifiedBAM**: èµ„æºå—é™åœºæ™¯ä½¿ç”¨
- **UCDC**: ç”¨äºç“¶é¢ˆå±‚å’Œè§£ç å™¨é˜¶æ®µï¼Œéœ€è¦å¤šå°ºåº¦ä¸Šä¸‹æ–‡ä¿¡æ¯æ—¶ä½¿ç”¨

### Q2: æ¨¡å—æ˜¯å¦æ”¯æŒå¯å˜è¾“å…¥å°ºå¯¸ï¼Ÿ

**A:** æ˜¯çš„ï¼æ‰€æœ‰æ¨¡å—éƒ½æ”¯æŒä»»æ„è¾“å…¥å°ºå¯¸ï¼Œæ— éœ€é¢„å…ˆæŒ‡å®šã€‚è¾“å…¥å¯ä»¥æ˜¯ä»»æ„ (B, C, H, W) å½¢çŠ¶ã€‚

### Q3: å¦‚ä½•è°ƒæ•´æ³¨æ„åŠ›å¼ºåº¦ï¼Ÿ

**A:** 
- è°ƒæ•´ `reduction_ratio` å‚æ•°ï¼ˆè¾ƒå°çš„å€¼ = æ›´å¼ºçš„æ³¨æ„åŠ›ï¼‰
- åœ¨ `ConvAttention` ä¸­ï¼Œ`gamma` å‚æ•°æ§åˆ¶æ³¨æ„åŠ›è¾“å‡ºçš„æƒé‡ï¼ˆå¯è®­ç»ƒï¼‰

### Q4: æ¨¡å—æ˜¯å¦æ”¯æŒæ‰¹å¤„ç†ï¼Ÿ

**A:** æ˜¯çš„ï¼æ‰€æœ‰æ¨¡å—éƒ½æ”¯æŒä»»æ„æ‰¹æ¬¡å¤§å°ã€‚

### Q5: å¦‚ä½•é›†æˆåˆ°ç°æœ‰ç½‘ç»œä¸­ï¼Ÿ

**A:** ç›´æ¥å°†æ¨¡å—æ’å…¥åˆ°ä½ çš„ç½‘ç»œä¸­å³å¯ï¼Œä¾‹å¦‚ï¼š
```python
class MyNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv = nn.Conv2d(3, 64, 3, padding=1)
        self.attention = ConvAttention(64)  # å³æ’å³ç”¨
        self.out = nn.Conv2d(64, 1, 1)
    
    def forward(self, x):
        x = self.conv(x)
        x = self.attention(x)  # æ’å…¥æ³¨æ„åŠ›
        x = self.out(x)
        return x
```

### Q6: æ¨¡å—çš„è®¡ç®—å¤æ‚åº¦å¦‚ä½•ï¼Ÿ

**A:** 
- **BilinearAttention**: O(C Ã— H Ã— W)ï¼Œè½»é‡çº§
- **ConvAttention**: O(C Ã— H Ã— W Ã— KÂ²)ï¼ŒK æ˜¯å·ç§¯æ ¸å¤§å°
- **ConvTransformerBlock**: O(C Ã— H Ã— W Ã— KÂ²)ï¼ŒåŒ…å«æ³¨æ„åŠ›+å‰é¦ˆ
- **SimplifiedBAM**: O(C Ã— H Ã— W)ï¼Œæœ€è½»é‡çº§
- **UCDC**: O(C Ã— H Ã— W Ã— KÂ²)ï¼ŒåŒ…å«å¤šå°ºåº¦æ‰©å¼ å·ç§¯å’Œskip connections

### Q7: æ˜¯å¦éœ€è¦é¢å¤–çš„ä¾èµ–ï¼Ÿ

**A:** ä¸éœ€è¦ï¼æ¨¡å—ä»…ä½¿ç”¨ PyTorch æ ‡å‡†åº“ï¼Œæ— éœ€å®‰è£…å…¶ä»–ä¾èµ–ã€‚

### Q8: æ¨¡å—æ˜¯å¦æ”¯æŒ GPUï¼Ÿ

**A:** æ˜¯çš„ï¼æ¨¡å—å®Œå…¨æ”¯æŒ GPUï¼Œåªéœ€å°†è¾“å…¥å¼ é‡ç§»åŠ¨åˆ° GPU ä¸Šï¼š
```python
device = torch.device('cuda')
x = torch.randn(2, 64, 32, 32).to(device)
module = BilinearAttention(64).to(device)
out = module(x)
```

---

## æ€»ç»“ (Summary)

`ABCNet_block.py` æä¾›äº†å®Œæ•´çš„ ABC ç½‘ç»œæ³¨æ„åŠ›æ¨¡å—å®ç°ï¼Œå…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š

âœ… **çœŸæ­£çš„å³æ’å³ç”¨**ï¼šæ— éœ€é…ç½®ï¼Œç›´æ¥ä½¿ç”¨  
âœ… **è‡ªåŠ¨é€‚é…**ï¼šæ”¯æŒä»»æ„è¾“å…¥å°ºå¯¸  
âœ… **é«˜æ•ˆå®ç°**ï¼šä¼˜åŒ–çš„è®¡ç®—æµç¨‹  
âœ… **æ˜“äºé›†æˆ**ï¼šå¯ä»¥è½»æ¾æ’å…¥åˆ°ç°æœ‰ç½‘ç»œ  
âœ… **æ— å¤–éƒ¨ä¾èµ–**ï¼šä»…ä½¿ç”¨ PyTorch æ ‡å‡†åº“  

`ABCNet_block.py` provides complete ABC network attention module implementation with:

âœ… **True plug-and-play**: No configuration needed, use directly  
âœ… **Auto adaptation**: Supports arbitrary input sizes  
âœ… **Efficient implementation**: Optimized computation flow  
âœ… **Easy integration**: Can be easily inserted into existing networks  
âœ… **No external dependencies**: Only uses PyTorch standard library  

---

## è®¸å¯è¯ (License)

è¯·å‚è€ƒé¡¹ç›®ä¸» LICENSE æ–‡ä»¶ã€‚

Please refer to the main LICENSE file of the project.

---

## è”ç³»æ–¹å¼ (Contact)

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·æäº¤ Issue æˆ– Pull Requestã€‚

For questions or suggestions, please submit an Issue or Pull Request.

